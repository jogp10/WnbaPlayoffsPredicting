{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project - WNBA Playoffs Prediction - G24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our data\n",
    "Basketball tournaments are usually split in two parts. First, all teams play each other aiming to achieve the greatest number of wins possible. Then, at the end of the first part of the season, a pre determined number of teams which were able to win the most games are qualified to the playoff season, where they play series of knock-out matches for the trophy.\n",
    "\n",
    "For the 10 years, data from players, teams, coaches, games and several other metrics were gathered and arranged on this dataset. The goal is to use this data to predict which teams will qualify for the playoffs in the next season.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Competition Format\n",
    "The 12 teams in the WNBA are split into an Eastern Conference and a Western Conference. WNBA fixtures begin with preseason games in May before each team plays 20 home games and 20 road games during the regular season.\n",
    "\n",
    "The aim for every team is to qualify for the Playoffs, which begin in September each year.\n",
    "\n",
    "The WNBA teams with the eight best regular season records regardless of standing qualify for the Playoffs. Higher seeds matchup with lower seeds, so the top seed faces the eight seed, the second seed faces the seven seed and so on.\n",
    "\n",
    "When it comes to betting on the Playoffs, the first round are best-of-three series. The semifinals and final are both best-of-five, meaning WNBA teams need to record three wins to claim victory in the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a free service to host our database. The Database is in PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Credentials\n",
    "\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "host = config[\"db_host\"]\n",
    "user = config[\"db_user\"]\n",
    "password = config[\"db_password\"]\n",
    "database = config[\"db_database\"]\n",
    "schema = config[\"db_schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "def execute(query):\n",
    "    cursor.execute(query)\n",
    "    connection.commit()\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def fetch(query):\n",
    "    cursor.execute(query)\n",
    "    return cursor.fetchall()\n",
    "\n",
    "SELECT = \"SELECT * FROM \" + schema + \".\" # + table_name \n",
    "INSERT = \"INSERT INTO \" + schema + \".\" # + table_name + \" VALUES \" + values\n",
    "UPDATE = \"UPDATE \" + schema + \".\" # + table_name + \" SET \" + column_name + \" = \" + value\n",
    "DELETE = \"DELETE FROM \" + schema + \".\"  # + table_name + \" WHERE \" + column_name + \" = \" + value\n",
    "\n",
    "# Test data, year 10\n",
    "TEST_DATA = \"year = 10\"\n",
    "TRAIN_DATA = \"year < 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The data about the players, teams and coaches consist of following relations:\n",
    "\n",
    "    awards_players (96 objects) - each record describes awards and prizes received by players across 10 seasons,\n",
    "    coaches (163 objects) - each record describes all coaches who've managed the teams during the time period,\n",
    "    players (894 objects) - each record contains details of all players,\n",
    "    players_teams (1877 objects) - each record describes the performance of each player for each team they played,\n",
    "    series_post (71 objects) - each record describes the series' results,\n",
    "    teams (143 objects) - each record describes the performance of the teams for each season,\n",
    "    teams_post (81 objects) - each record describes the results of each team at the post-season.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_players = fetch(SELECT + \"awards_players\") # awards and prizes received by players across 10 seasons,\n",
    "coaches = fetch(SELECT + \"coaches\") # all coaches who've managed the teams during the time period,\n",
    "players = fetch(SELECT + \"players\") # details of all players,\n",
    "players_teams = fetch(SELECT + \"players_teams\") # performance of each player for each team they played,\n",
    "series_post = fetch(SELECT + \"series_post\") # series' results,\n",
    "teams = fetch(SELECT + \"teams\") # performance of the teams for each season,\n",
    "teams_post = fetch(SELECT + \"teams_post\") # results of each team at the post-season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data in a dataframe\n",
    "awards_players_df = pd.DataFrame(awards_players, columns=['playerID', 'award', 'year', 'lgID'])\n",
    "coaches_df = pd.DataFrame(coaches, columns=['coachID', 'year', 'tmID', 'lgID', 'stint', 'won', 'lost', 'post_wins', 'post_losses'])\n",
    "players_df = pd.DataFrame(players, columns=['bioID', 'pos', 'firstseason', 'lastseason', 'height', 'weight', 'college', 'collegeOther', 'birthDate', 'deathDate'])\n",
    "players_teams_df = pd.DataFrame(players_teams, columns=['playerID', 'year', 'stint', 'tmID', 'lgID', 'GP', 'GS', 'minutes', 'points', 'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade', 'dq', 'PostGP', 'PostGS', 'PostMinutes', 'PostPoints', 'PostoRebounds', 'PostdRebounds', 'PostRebounds', 'PostAssists', 'PostSteals', 'PostBlocks', 'PostTurnovers', 'PostPF', 'PostfgAttempted', 'PostfgMade', 'PostftAttempted', 'PostftMade', 'PostthreeAttempted', 'PostthreeMade', 'PostDQ'])\n",
    "series_post_df = pd.DataFrame(series_post, columns=['year', 'round', 'series', 'tmIDWinner', 'lgIDWinner', 'tmIDLoser', 'lgIDLoser', 'W', 'L'])\n",
    "teams_df = pd.DataFrame(teams, columns=['year', 'lgID', 'tmID', 'franchID', 'confID', 'divID', 'rank', 'playoff', 'seeded', 'firstRound', 'semis', 'finals', 'name', 'o_fgm', 'o_fga', 'o_ftm', 'o_fta', 'o_3pm', 'o_3pa', 'o_oreb', 'o_dreb', 'o_reb', 'o_asts', 'o_pf', 'o_stl', 'o_to', 'o_blk', 'o_pts', 'd_fgm', 'd_fga', 'd_ftm', 'd_fta', 'd_3pm', 'd_3pa', 'd_oreb', 'd_dreb', 'd_reb', 'd_asts', 'd_pf', 'd_stl', 'd_to', 'd_blk', 'd_pts', 'tmORB', 'tmDRB', 'tmTRB', 'opptmORB', 'opptmDRB', 'opptmTRB', 'won', 'lost', 'GP', 'homeW', 'homeL', 'awayW', 'awayL', 'confW', 'confL', 'min', 'attend', 'arena'])\n",
    "teams_post_df = pd.DataFrame(teams_post, columns=['year', 'tmID', 'lgID', 'W', 'L'])\n",
    "\n",
    "#make a dictionary with all the dataframes\n",
    "dfs = {'awards_players_df': awards_players_df, 'coaches_df': coaches_df, 'players_df': players_df, 'players_teams_df': players_teams_df, 'series_post_df': series_post_df, 'teams_df': teams_df, 'teams_post_df': teams_post_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with a Exploratory Data Analysis. There are 7 different tables, with different sizes both in lines and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each table, print the table name, number of rows and columns\n",
    "for df in dfs:\n",
    "    print(df)\n",
    "    print(dfs[df].shape,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping into the data analysis, one of the first things we noticed is that all the tables have an \"League ID\" (lgID) attribute. As the WNBA is the only league we are covering, and there is no variability in this column, we can drop it, as well as other columns that may have always the same content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns whose values are always the same\n",
    "for df in dfs:\n",
    "    for col in dfs[df].columns:\n",
    "        if len(dfs[df][col].unique()) == 1:\n",
    "            print(df, col)\n",
    "            dfs[df].drop(col, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, we also noted there are three dead players in the players table. We should take that into account when doing the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print dead players\n",
    "print(players_df[players_df['deathDate'] != '0000-00-00'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, there are players that have not played any season of the seasons given. We should take that into account when doing the analysis. There are 338 players that have not played any season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#players that have not played in the last 10 years\n",
    "fetch(\"SELECT p.bioid FROM wnba.players p WHERE p.bioid not in (select pt.playerid  from wnba.players_teams pt)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Describing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, we printed the head of each table, to get a sense of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do a head of each table\n",
    "for df in dfs:\n",
    "    print(df)\n",
    "    display(dfs[df].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having an idea of what the data looks like, we now want to see a more detailed description to get more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do a describe of each table\n",
    "for df in dfs:\n",
    "    print(df)\n",
    "    display(dfs[df].describe(include='all'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  # do a info of each table\n",
    "for df in dfs:\n",
    "    print(df)\n",
    "    print(dfs[df].info(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the info and describes we understand that:\n",
    "- There are no Null entries (although there values that are simply an empty string), as we will also prove next.\n",
    "- There are some columns with the DataType \"object\", most of them being strings.\n",
    "- There are binary objects (like confID and playoff, in the 'teams' table, with the values \"Y\" or \"N\") that could be substituted by a binary, as well as ternary objects (like the firstRound, semis and finals in the 'teams' table, with the values \"W\", \"L\" or \"\") that could also be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do a isnull of each table to see if there are null values\n",
    "for df in dfs:\n",
    "    print(df)\n",
    "    print(dfs[df].isnull().sum(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to see what were the different values for the objects and check if there were more objects that could be transformed into binary and ternary variables, so we printed the objects unique values and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the value counts for each column of type object\n",
    "for df in dfs:\n",
    "    print(df)\n",
    "    for col in dfs[df].columns:\n",
    "        if dfs[df][col].dtype == 'object':\n",
    "            print(dfs[df][col].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We conclude the previous identified variables are the only objects with 2 or 3 unique values (confID and playoff, in the 'teams' table, with the values \"Y\" or \"N\" and the firstRound, semis and finals in the 'teams' table, with the values \"W\", \"L\" or \"\").\n",
    "- There are players with no position and no college assigned (\"\").\n",
    "- There are players with no date of birth in the record (0000-00-00).\n",
    "- There is the need to do null value uniformization, as there are some columns with empty strings, others with default 0 values and other values that represent null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We moved on to the numerical variables, trying to find outliers using boxplots (as we have a lot of variables, we only run for the ones we thought had outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    #print(df)\n",
    "    \n",
    "    # do box plots of the numerical columns\n",
    "    for col in dfs[df].columns:\n",
    "        if dfs[df][col].dtype != 'object':\n",
    "            if col == 'height' or col == 'weight':\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                sns.boxplot(x=dfs[df][col])\n",
    "                plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at the boxplots, we can understand that the height and weight variables have default 0 values and should be treated as null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wins and losses by team, sorted by number of wins: stacked bar chart\n",
    "wins_by_team = teams_df.groupby('name')['won'].sum().sort_values(ascending=False)\n",
    "losses_by_team = teams_df.groupby('name')['lost'].sum().sort_values(ascending=False)\n",
    "wins_by_team = wins_by_team.reset_index()\n",
    "losses_by_team = losses_by_team.reset_index()\n",
    "wins_by_team['lost'] = losses_by_team['lost']\n",
    "wins_by_team = wins_by_team.sort_values(by='won', ascending=False)\n",
    "wins_by_team = wins_by_team.set_index('name')\n",
    "wins_by_team.plot(kind='bar', stacked=True, figsize=(20, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of games played by each team differs (there may be teams that are no longer playing), so we can't compare the number of wins and losses directly. We need to calculate the percentage of wins and losses for each team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#win percentage by team, sorted by win percentage: bar chart with color gradient, with a horizontal line at the league average\n",
    "wins_by_team['win_percentage'] = wins_by_team['won'] / (wins_by_team['won'] + wins_by_team['lost'])\n",
    "wins_by_team = wins_by_team.sort_values(by='win_percentage', ascending=False)\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=wins_by_team.index, y=wins_by_team['win_percentage'], palette='rocket')\n",
    "plt.axhline(wins_by_team['win_percentage'].mean(), color='black')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In terms of win percentage, it seems like a competitive league, with more than half of the teams having a win percentage of 50% or more, taking advantage of the worst teams. There is also just one team below 40% of wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw that there were teams it more games than the others, we want to see which teams played in which seasons, to see if there are teams that are no longer playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumerate the seasons a team played in the league\n",
    "seasons_by_team = teams_df.groupby('name')['year']\n",
    "print(seasons_by_team.unique())\n",
    "print(\"-----------------------\")\n",
    "print(seasons_by_team.nunique())\n",
    "#print number of seasons played\n",
    "print(\"-----------------------\")\n",
    "#filter only teams that have played in the last season\n",
    "teams_last_season = teams_df[teams_df['year'] == 10]\n",
    "print(teams_last_season['name'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are tables with a lot of attributes (player_teams 43 and teams_df 61, for example). We will evaluate the correlations and delete the most correlated pairs of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CORRELATION = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_most_correlated(df):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "\n",
    "    correlation_matrix = df_copy.corr()\n",
    "\n",
    "    sorted_correlations = correlation_matrix.unstack().sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "    # Get the pairs of attributes with the highest correlation values\n",
    "    most_correlated_pairs = sorted_correlations[sorted_correlations > MAX_CORRELATION]\n",
    "    most_correlated_pairs = most_correlated_pairs[most_correlated_pairs < 1.0]\n",
    "\n",
    "\n",
    "    #delete repeated pairs (e.g. (a,b) and (b,a))\n",
    "    most_correlated_pairs = most_correlated_pairs[::2]\n",
    "    print(most_correlated_pairs)\n",
    "\n",
    "    #drop the attributes with the highest correlation values\n",
    "    for pair in most_correlated_pairs.index:\n",
    "        if pair[0] in df_copy.columns:\n",
    "            df_copy.drop(pair[0], inplace=True, axis=1)\n",
    "        if pair[1] in df_copy.columns:\n",
    "            df_copy.drop(pair[1], inplace=True, axis=1)\n",
    "\n",
    "    #print the attributes that were dropped\n",
    "    print(df_copy.columns)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the function for all the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    print(df)\n",
    "    #select only the numerical columns\n",
    "    #to make the df actually change, we have to assign it to the df\n",
    "    #df = delete_most_correlated(dfs[df])\n",
    "    delete_most_correlated(dfs[df].select_dtypes(include=np.number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with that we end our understanding phase.\n",
    "Our main takeaways are:\n",
    "- There are dead players in the players table. We should take that into account when doing the analysis.\n",
    "- There are players that have not played any season of the seasons given. We should take that into account when doing the analysis. There are 338 players that have not played any season.\n",
    "- There are no Null entries (although there values that are simply an empty string)\n",
    "- There are some columns with the DataType \"object\", most of them being strings.\n",
    "- There are binary objects (like confID and playoff, in the 'teams' table, with the values \"Y\" or \"N\") that could be substituted by a binary, as well as ternary objects (like the firstRound, semis and finals in the 'teams' table, with the values \"W\", \"L\" or \"\") that could also be transformed.\n",
    "- There are players with no position and no college assigned (\"\").\n",
    "- There are players with no date of birth in the record (0000-00-00).\n",
    "- There is the need to do null value uniformization, as there are some columns with empty strings, others with default 0 values and other values that represent null.\n",
    "- The height and weight variables have default 0 values and should be treated as null values.\n",
    "- The number of games played by each team differs (there may be teams that are no longer playing), so we can't compare the number of wins and losses directly. Win percentage should be used.\n",
    "- In terms of win percentage, it seems like a competitive league, with more than half of the teams having a win percentage of 50% or more, taking advantage of the worst teams. There is also just one team below 40% of wins.\n",
    "- There are teams that are no longer playing.\n",
    "- There are a lot of highly correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Merging Tables (FALTAM COISAS ANTES MAS VOU VER SE DÀ PARA IR PENSANDO NISTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Merged tables:\n",
    "- teams with teams_post, some values are NaN when the team didn't participate in the playoffs\n",
    "- players with players_teams, some values are NaN when the player didn't participate in any team, remove such cases?\n",
    "- awards with players\n",
    "\n",
    "- próximas ideia... listas de valores agregados ou colunas a 0 ou 1 se forem poucos valores diferentes\n",
    "- acrescentar coluna com pontuação a cada jogador e depois somar essas pontuações em teams\n",
    "- Is series_post even relevant??\n",
    "- se se mergirem os coaches, fazer a média dos resultados quando há mais q 1 por ano por equipa? criar alguma espécie de pontuação em vez das colunas todas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "teams_post_df = teams_post_df.rename(columns={'W': 'W_post', 'L': 'L_post'})\n",
    "teams_post_df.head()\n",
    "teams_df = teams_df.merge(teams_post_df, on=['tmID', 'year'], how='left')\n",
    "\n",
    "teams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join players (bioID) with players_teams (playerID)\n",
    "\n",
    "players_teams_df = players_teams_df.rename(columns={'playerID': 'bioID'})\n",
    "players_teams_df.head()\n",
    "players_df = players_df.merge(players_teams_df, on=['bioID'], how='left')\n",
    "players_df = players_df.rename(columns={'bioID': 'playerID'})\n",
    "players_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# merge players with awards on playerID and year\n",
    "awards_players_df.head()\n",
    "players_df = players_df.merge(awards_players_df, on=['playerID', 'year'], how='left')\n",
    "#players_df.boxplot(column='points', by='award')\n",
    "players_df['award'].value_counts()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(players_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###### VER SE EXDISTE RELAÇÂO DE teams_post e series_post com playoffs do ano a seguir\n",
    "\n",
    "series_df_1 = series_post_df[['year', 'series', 'tmIDWinner']]\n",
    "series_df_1 = series_df_1.rename(columns={'tmIDWinner': 'tmID'})\n",
    "# rename year to year-1\n",
    "series_df_1['year'] = series_df_1['year'] + 1 # we will compare the current year with the past performance\n",
    "series_df_1.head()\n",
    "\n",
    "teams_df_1 = teams_df[['year', 'tmID', 'playoff']]\n",
    "teams_df_1.head()\n",
    "\n",
    "series_A_df = series_df_1[series_df_1['series'] == 'A']\n",
    "series_A_merged = (series_A_df\n",
    "                   .merge(teams_df_1, on=['year', 'tmID'], how='left'))\n",
    "series_A_merged.head()\n",
    "\n",
    "series_A_merged['playoff'].value_counts()\n",
    "\n",
    "series_B_df = series_df_1[series_df_1['series'] == 'B']\n",
    "series_B_merged = (series_B_df\n",
    "                   .merge(teams_df_1, on=['year', 'tmID'], how='left'))\n",
    "\n",
    "series_C_df = series_df_1[series_df_1['series'] == 'C']\n",
    "series_C_merged = (series_C_df\n",
    "                   .merge(teams_df_1, on=['year', 'tmID'], how='left'))\n",
    "\n",
    "series_D_df = series_df_1[series_df_1['series'] == 'D']\n",
    "series_D_merged = (series_D_df\n",
    "                   .merge(teams_df_1, on=['year', 'tmID'], how='left'))\n",
    "\n",
    "series_E_df = series_df_1[series_df_1['series'] == 'E']\n",
    "series_E_merged = (series_E_df\n",
    "                   .merge(teams_df_1, on=['year', 'tmID'], how='left'))\n",
    "\n",
    "series_F_df = series_df_1[series_df_1['series'] == 'F']\n",
    "series_F_merged = (series_F_df\n",
    "                   .merge(teams_df_1, on=['year', 'tmID'], how='left'))\n",
    "\n",
    "series_G_df = series_df_1[series_df_1['series'] == 'G']\n",
    "series_G_merged = (series_G_df\n",
    "                   .merge(teams_df_1, on=['year', 'tmID'], how='left'))\n",
    "\n",
    "# now evaluate the relation between qualifying for the playoffs and winning in series A, B, C ... G\n",
    "\n",
    "# for each merged df, calculate the number of teams that qualified for the playoffs and the number of teams that didn't qualify for the playoffs and compare in a plot\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(2, 4, 1)\n",
    "sns.countplot(x='playoff', data=series_A_merged)\n",
    "plt.title('Series A')\n",
    "plt.subplot(2, 4, 2)\n",
    "sns.countplot(x='playoff', data=series_B_merged)\n",
    "plt.title('Series B')\n",
    "plt.subplot(2, 4, 3)\n",
    "sns.countplot(x='playoff', data=series_C_merged)\n",
    "plt.title('Series C')\n",
    "plt.subplot(2, 4, 4)\n",
    "sns.countplot(x='playoff', data=series_D_merged)\n",
    "plt.title('Series D')\n",
    "plt.subplot(2, 4, 5)\n",
    "sns.countplot(x='playoff', data=series_E_merged)\n",
    "plt.title('Series E')\n",
    "plt.subplot(2, 4, 6)\n",
    "sns.countplot(x='playoff', data=series_F_merged)\n",
    "plt.title('Series F')\n",
    "plt.subplot(2, 4, 7)\n",
    "sns.countplot(x='playoff', data=series_G_merged)\n",
    "plt.title('Series G')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# merge teams_df with players_df\n",
    "players_df.head()\n",
    "\n",
    "# for each team in teams_df, calculate the average of each player's performance in the team, for every column in players_df\n",
    "teams_df2 = teams_df.copy()\n",
    "for col in players_df:\n",
    "    if players_df[col].dtype == 'float64' and col not in {'year', 'tmID', 'lgID', 'playerID', 'firstseason', 'lastseason'}:\n",
    "        print(col)\n",
    "        print(players_df.groupby(['year', 'tmID'])[col].mean())\n",
    "        print('-----------------------')\n",
    "        \n",
    "#         add the average to teams_df\n",
    "        teams_df2 = teams_df.merge(players_df.groupby(['year', 'tmID'])[col].mean(), on=['year', 'tmID'], how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(teams_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(teams_df2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
