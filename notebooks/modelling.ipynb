{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843b0d3d",
   "metadata": {},
   "source": [
    "# Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ac332",
   "metadata": {},
   "source": [
    "## Get the data from the previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ebe47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.492929300Z",
     "start_time": "2023-11-05T23:33:50.115650400Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the data from the previous notebook, as csv files, and load them into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MERGED_DATA = True\n",
    "\n",
    "awards_players_df = pd.read_csv('../prep_data/awards_players_df.csv')\n",
    "coaches_df = pd.read_csv('../prep_data/coaches_df.csv')\n",
    "players_df = pd.read_csv('../prep_data/players_df.csv')\n",
    "players_teams_df = pd.read_csv('../prep_data/players_teams_df.csv')\n",
    "series_post_df = pd.read_csv('../prep_data/series_post_df.csv')\n",
    "if USE_MERGED_DATA:\n",
    "    teams_df = pd.read_csv('../prep_data/prepared_dataset.csv')\n",
    "else:\n",
    "    teams_df = pd.read_csv('../prep_data/teams_df.csv')\n",
    "teams_post_df = pd.read_csv('../prep_data/teams_post_df.csv')\n",
    "\n",
    "# Make a dictionary with all the dataframes\n",
    "dfs = {'awards_players_df': awards_players_df, 'coaches_df': coaches_df, 'players_df': players_df, 'players_teams_df': players_teams_df, 'series_post_df': series_post_df, 'teams_df': teams_df, 'teams_post_df': teams_post_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the data from competition year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Credentials\n",
    "with open(\"../config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "host = config[\"db_host\"]\n",
    "user = config[\"db_user\"]\n",
    "password = config[\"db_password\"]\n",
    "database = config[\"db_database\"]\n",
    "schema = config[\"db_11_schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "def execute(query):\n",
    "    cursor.execute(query)\n",
    "    connection.commit()\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def fetch(query):\n",
    "    cursor.execute(query)\n",
    "    return cursor.fetchall()\n",
    "\n",
    "SELECT = \"SELECT * FROM \" + schema + \".\" # + table_name \n",
    "INSERT = \"INSERT INTO \" + schema + \".\" # + table_name + \" VALUES \" + values\n",
    "UPDATE = \"UPDATE \" + schema + \".\" # + table_name + \" SET \" + column_name + \" = \" + value\n",
    "DELETE = \"DELETE FROM \" + schema + \".\"  # + table_name + \" WHERE \" + column_name + \" = \" + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coaches = fetch(SELECT + \"coaches\") # all coaches who've managed the teams during the time period,\n",
    "players_teams = fetch(SELECT + \"players_teams\") # performance of each player for each team they played,\n",
    "teams = fetch(SELECT + \"teams\") # performance of the teams for each season,\n",
    "\n",
    "players_teams_11_df = pd.DataFrame(players_teams, columns=['playerID', 'year', 'stint', 'tmID', 'lgID'])\n",
    "teams_11_df = pd.DataFrame(teams, columns=['year', 'lgID', 'tmID', 'franchID', 'confID', 'name', 'arena', 'playoff'])\n",
    "coaches_11_df = pd.DataFrame(coaches, columns=['coachID', 'year', 'tmID', 'lgID', 'stint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make some preprocessing to year 11 data to make it compatible with the other years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = ['confID', 'playoff']\n",
    "\n",
    "for col in binary_columns:\n",
    "    teams_11_df[col] = teams_11_df[col].replace('EA', 0)\n",
    "    teams_11_df[col] = teams_11_df[col].replace('WE', 1)\n",
    "    teams_11_df[col] = teams_11_df[col].replace('N', 0)\n",
    "    teams_11_df[col] = teams_11_df[col].replace('Y',1)\n",
    "\n",
    "\n",
    "players_teams_11_df = players_teams_11_df.reindex(columns=players_teams_df.columns)\n",
    "players_teams_df = pd.concat([players_teams_11_df, players_teams_df])\n",
    "\n",
    "teams_11_df = teams_11_df.reindex(columns=teams_df.columns)\n",
    "teams_df = pd.concat([teams_11_df, teams_df])\n",
    "\n",
    "coaches_11_df = coaches_11_df.reindex(columns=coaches_df.columns)\n",
    "coaches_df = pd.concat([coaches_11_df, coaches_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676924c",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, constants and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some constants that we will be going to use across the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAMES = ['No Playoffs', 'Playoffs']\n",
    "N_FOLDS_CV = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create reusable functions that will be useful across the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d27304e9ece233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.683928100Z",
     "start_time": "2023-11-05T23:33:50.164392900Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(cm, target_names):\n",
    "    df_cm = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "    sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "def print_results(clf, X_train, X_test, y_train, y_test, output=True):\n",
    "    # Fit the classifier on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    if output:\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Create a confusion matrix to evaluate the model and print with labels\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    if output:\n",
    "        print_confusion_matrix(cm, TARGET_NAMES)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def cross_validation(clf, X, y, output=True):\n",
    "    cv = StratifiedKFold(n_splits=N_FOLDS_CV, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Display the cross-validation results\n",
    "    if output:\n",
    "        print(\"Cross-Validation Results:\")\n",
    "        print(\"Mean Accuracy: {:.2f}%\".format(scores.mean() * 100))\n",
    "        print(\"Standard Deviation: {:.2f}\".format(scores.std()))\n",
    "\n",
    "    return scores\n",
    "    \n",
    "def force_qualify_8_teams(y_pred, y_test, confIDs, output=True):  \n",
    "    # Split the predictions and test labels by conference\n",
    "    y_pred_conf0 = y_pred[confIDs == 0]\n",
    "    y_pred_conf1 = y_pred[confIDs == 1]\n",
    "    y_test_conf0 = y_test[confIDs == 0]\n",
    "    y_test_conf1 = y_test[confIDs == 1]\n",
    "\n",
    "    # Get the indices of the top 4 teams in each conference\n",
    "    top_teams_indices_conf0 = y_pred_conf0.argsort()[-4:][::-1]\n",
    "    top_teams_indices_conf1 = y_pred_conf1.argsort()[-4:][::-1]\n",
    "\n",
    "    # Convert the labels to numeric format (0 for 'No Playoff', 1 for 'Playoff')\n",
    "    y_pred_numeric_conf0 = [1 if i in top_teams_indices_conf0 else 0 for i in range(len(y_pred_conf0))]\n",
    "    y_pred_numeric_conf1 = [1 if i in top_teams_indices_conf1 else 0 for i in range(len(y_pred_conf1))]\n",
    "\n",
    "    # Concatenate the predictions and test labels for both conferences\n",
    "    y_pred_numeric = np.concatenate([y_pred_numeric_conf0, y_pred_numeric_conf1])\n",
    "    y_test = np.concatenate([y_test_conf0, y_test_conf1])\n",
    "\n",
    "    # Calculate the accuracy of the modified predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred_numeric)\n",
    "    if output:\n",
    "        print(\"Modified Accuracy (Top 4 Teams from Each Conference as Playoff):\", accuracy)\n",
    "    \n",
    "    # Create a confusion matrix to evaluate the model and print with labels\n",
    "    cm = confusion_matrix(y_test, y_pred_numeric)\n",
    "    if output:\n",
    "        print_confusion_matrix(cm, TARGET_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f045b",
   "metadata": {},
   "source": [
    "## With current year info prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df3c68",
   "metadata": {},
   "source": [
    "Run a simple model on the teams table to predict if a team will make it to the playoffs or not. Let's see how it behaves.\n",
    "\n",
    "The main objective of this chapter is to find which is the model that performs the best, given the perfect data (the data from the year to be predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e9886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.736022300Z",
     "start_time": "2023-11-05T23:33:50.172181100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train years and test year (year 10)\n",
    "teams_train_df = teams_df[teams_df['year'] < 10]\n",
    "teams_test_df = teams_df[teams_df['year'] == 10]\n",
    "\n",
    "cross_val_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c03a8a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.795542700Z",
     "start_time": "2023-11-05T23:33:50.192253800Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = teams_train_df['playoff']\n",
    "y_test = teams_test_df['playoff']\n",
    "confIDs = teams_test_df['confID']\n",
    "X_train = teams_train_df.drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "X_test = teams_test_df.drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "\n",
    "if USE_MERGED_DATA:\n",
    "    X_train = X_train.drop(['W_post', 'L_post'], axis=1)\n",
    "    X_test = X_test.drop(['W_post', 'L_post'], axis=1)\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "y_pred = print_results(clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2eafc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.954082900Z",
     "start_time": "2023-11-05T23:33:50.493928300Z"
    }
   },
   "outputs": [],
   "source": [
    "force_qualify_8_teams(y_pred, y_test, confIDs)\n",
    "\n",
    "scores = cross_validation(clf, X, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "y_pred = print_results(knn_classifier, X_train, X_test, y_train, y_test)\n",
    "\n",
    "force_qualify_8_teams(y_pred, y_test, confIDs)\n",
    "\n",
    "scores = cross_validation(knn_classifier, X, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "y_pred = print_results(knn_classifier, X_train, X_test, y_train, y_test)\n",
    "\n",
    "force_qualify_8_teams(y_pred, y_test, confIDs)\n",
    "\n",
    "scores = cross_validation(mnb_classifier, X, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\n",
    "    \n",
    "y_pred = print_results(mlp_classifier, X_train, X_test, y_train, y_test)\n",
    "\n",
    "force_qualify_8_teams(y_pred, y_test, confIDs)\n",
    "\n",
    "scores = cross_validation(mlp_classifier, X, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the current year information, the best model is the Random Forest, with a 0.85 accuracy. This permited as to see which models will have a better performance with the data from the previous years, when we don't have the current year information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(cross_val_scores, labels=['Random Forest', 'KNN', 'Multinomial NB', 'MLP'])\n",
    "plt.title('Cross-Validation Scores')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier seems to be performing better than the other classifiers.\n",
    "\n",
    "In the next chapter we will explore using a rolling window to use past years for the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2556d35",
   "metadata": {},
   "source": [
    "## Rolling window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efd014",
   "metadata": {},
   "source": [
    "We will use a rolling window to predict the next season. We will use the last 3 seasons to predict the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a78b5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.980071300Z",
     "start_time": "2023-11-05T23:33:50.733839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the rolling window size (number of past years to consider)\n",
    "ROLLING_WIN_SIZE = 3  # You can adjust this value\n",
    "PREDICTION_YEAR = 11\n",
    "\n",
    "historical_data_df = teams_df[teams_df['year'] < PREDICTION_YEAR]\n",
    "\n",
    "def create_rolling_window_dataset(df, window_size):\n",
    "    rolling_window_data = []\n",
    "    for tmID, group in df.groupby('tmID'):\n",
    "        group = group.sort_values(by='year')\n",
    "        for i in range(len(group) - window_size + 1):\n",
    "            window = group.iloc[i:i + window_size]\n",
    "            rolling_window_data.append(window)\n",
    "    return rolling_window_data\n",
    "\n",
    "def generate_weights(window_size):\n",
    "    weights = np.linspace(0.1, 1, window_size)\n",
    "    weights /= weights.sum()  # Normalize weights to ensure they sum to 1\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the rolling window dataset.\n",
    "In this dataset, three years are combined, with more weight given to the most recent year, and combining that historic data with the target variable (playoff) from the target year (the n+1 year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_df():\n",
    "    # Create the rolling window datasets\n",
    "    rolling_window_historic = create_rolling_window_dataset(historical_data_df,  ROLLING_WIN_SIZE)\n",
    "\n",
    "    # Define weights (you can adjust these weights as needed)\n",
    "    weights = generate_weights(ROLLING_WIN_SIZE)\n",
    "\n",
    "    weighted_features = ['o_fta', 'o_3pm', 'o_oreb', 'o_asts', 'o_stl', 'o_to', 'o_blk', 'd_3pa', 'd_oreb', 'd_dreb', 'd_asts', 'd_stl', 'd_to', 'd_blk', 'attend']\n",
    "\n",
    "    if USE_MERGED_DATA:\n",
    "        weighted_features += ['blocks_weighted_mean', 'threeAttempted_weighted_mean', 'FG%_weighted_mean', 'FT%_weighted_mean', 'Avg_Assists_Per_Game_weighted_mean', 'Durability_Ratio_weighted_mean', 'POSITION_METRIC_weighted_mean', 'num_trophies']\n",
    "\n",
    "    # Create a new dataframe to store the rolling window data\n",
    "    rolling_data = pd.DataFrame(columns=teams_df.columns)\n",
    "\n",
    "    for window in rolling_window_historic:\n",
    "        #For each window, we will add a line to the rolling_data dataframe, as follows:\n",
    "        #1. Create a new empty line for the rolling_data dataframe\n",
    "        rolling_data_line = pd.DataFrame(columns=teams_df.columns)\n",
    "\n",
    "        #2. Get the weighted average of the weighted features\n",
    "        #Note: The weighted average is calculated as follows:\n",
    "        #weighted average = sum of (weight * feature value) / sum of weights\n",
    "        #For example, if the weights are [0.2, 0.3, 0.5] and the feature values are [10, 20, 30], then the weighted average is:\n",
    "        #weighted average = (0.2 * 10 + 0.3 * 20 + 0.5 * 30) / (0.2 + 0.3 + 0.5) = 24\n",
    "        for feature in weighted_features:\n",
    "            weighted_average = np.average(window[feature].values, weights=weights)\n",
    "            rolling_data_line[feature] = [weighted_average]\n",
    "\n",
    "        #3. Join that data with the data from the following year (the year we want to predict)\n",
    "        #Note: The data from the following year will be used as the target label, get next year from the teams_df dataframe\n",
    "        next_year = window['year'].max() + 1\n",
    "        next_year_data = teams_df[(teams_df['tmID'] == window['tmID'].values[0]) & (teams_df['year'] == next_year)]\n",
    "\n",
    "        #Add year, confID, tmID, and playoff columns\n",
    "        try:\n",
    "            rolling_data_line['year'] = next_year\n",
    "            rolling_data_line['tmID'] = window['tmID'].values[0]\n",
    "            rolling_data_line['confID'] = next_year_data['confID'].values[0]\n",
    "            rolling_data_line['playoff'] = next_year_data['playoff'].values[0]\n",
    "        except:\n",
    "            #If there is no data for the next year, skip this line\n",
    "            #print('No data for next year')\n",
    "            continue\n",
    "        \n",
    "        #4. Concat the data to the rolling_data dataframe\n",
    "        rolling_data = pd.concat([rolling_data, rolling_data_line])\n",
    "\n",
    "    rolling_data = rolling_data.reset_index(drop=True)\n",
    "    return rolling_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafde03cbdc1cd88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:52.246618Z",
     "start_time": "2023-11-05T23:33:50.748554200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rolling_data = rolling_window_df()\n",
    "\n",
    "y_train = rolling_data[rolling_data['year'] < PREDICTION_YEAR]['playoff'].astype(int)\n",
    "y_test = rolling_data[rolling_data['year'] == PREDICTION_YEAR]['playoff'].astype(int)\n",
    "confIDs = rolling_data[rolling_data['year'] == PREDICTION_YEAR]['confID']\n",
    "X_train = rolling_data[rolling_data['year'] < PREDICTION_YEAR].drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "X_test = rolling_data[rolling_data['year'] == PREDICTION_YEAR].drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "\n",
    "if USE_MERGED_DATA:\n",
    "    X_train = X_train.drop(['W_post', 'L_post'], axis=1)\n",
    "    X_test = X_test.drop(['W_post', 'L_post'], axis=1)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])\n",
    "\n",
    "display(X_train.head())\n",
    "\n",
    "y_pred = print_results(clf, X_train, X_test, y_train, y_test)\n",
    "\n",
    "force_qualify_8_teams(y_pred, y_test, confIDs)\n",
    "\n",
    "scores = cross_validation(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to find what are the best parameters for the Random Forest, KNN and M Naive Bayes models, using Grid Search, and tune this models to see if we can improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use grid search to find the best parameters\n",
    "cross_val_scores = []\n",
    "best_clf = {}\n",
    "\n",
    "parameters = {'n_estimators': [10, 50, 100], 'max_depth': [9, 11, 13], 'max_features': ['sqrt', 'log2', 0.2], 'criterion': ['gini', 'entropy']}\n",
    "clf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(clf, parameters, cv=5, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "clf = grid_search.best_estimator_\n",
    "best_clf['Random Forest'] = (grid_search.best_score_, grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best parameters to train the model\n",
    "y_pred = print_results(clf, X_train, X_test, y_train, y_test)\n",
    "force_qualify_8_teams(y_pred, y_test, confIDs)\n",
    "scores = cross_validation(clf, X, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use grid search to find the best parameters\n",
    "parameters = {'n_neighbors': [3, 5, 7, 9, 11, 13], 'weights': ['uniform', 'distance']}\n",
    "clf = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(clf, parameters, cv=5, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "clf = grid_search.best_estimator_\n",
    "best_clf['KNN'] = (grid_search.best_score_, grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the best parameters to train the model\n",
    "y_pred = print_results(clf, X_train, X_test, y_train, y_test)\n",
    "force_qualify_8_teams(y_pred, y_test, confIDs)\n",
    "scores = cross_validation(clf, X, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use grid search to find the best parameters\n",
    "parameters = {'alpha': [0.1, 0.5, 1, 2, 5, 10]}\n",
    "clf = MultinomialNB()\n",
    "grid_search = GridSearchCV(clf, parameters, cv=5, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "clf = grid_search.best_estimator_\n",
    "best_clf['Multinomial NB'] = (grid_search.best_score_, grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the best parameters to train the model\n",
    "y_pred = print_results(clf, X_train, X_test, y_train, y_test)\n",
    "force_qualify_8_teams(y_pred, y_test, confIDs)\n",
    "scores = cross_validation(clf, X, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the cross validation scores\n",
    "plt.boxplot(cross_val_scores, labels=['Random Forest', 'KNN', 'Multinomial NB'])\n",
    "plt.title('Cross-Validation Scores')\n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting our best classifier, we will try different numbers of ROLLING WINDOW to see if we can improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store the best score and estimator\n",
    "best_score = -1\n",
    "best_estimator = None\n",
    "best_clf_name = None\n",
    "\n",
    "# Iterate over the best_clf dictionary\n",
    "for clf_name, (score, estimator) in best_clf.items():\n",
    "    # If this classifier's score is higher than the current best score, update the best score and estimator\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_estimator = estimator\n",
    "        best_clf_name = clf_name\n",
    "\n",
    "print(f\"The best classifier is {best_clf_name} with a score of {best_score}\")\n",
    "\n",
    "cross_val_scores = []\n",
    "for i in range(1, 10):\n",
    "    ROLLING_WIN_SIZE = i\n",
    "    rolling_data = rolling_window_df()\n",
    "\n",
    "    y_train = rolling_data[rolling_data['year'] < PREDICTION_YEAR]['playoff'].astype(int)\n",
    "    y_test = rolling_data[rolling_data['year'] == PREDICTION_YEAR]['playoff'].astype(int)\n",
    "    confIDs = rolling_data[rolling_data['year'] == PREDICTION_YEAR]['confID']\n",
    "    X_train = rolling_data[rolling_data['year'] < PREDICTION_YEAR].drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "    X_test = rolling_data[rolling_data['year'] == PREDICTION_YEAR].drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "\n",
    "    if USE_MERGED_DATA:\n",
    "        X_train = X_train.drop(['W_post', 'L_post'], axis=1)\n",
    "        X_test = X_test.drop(['W_post', 'L_post'], axis=1)\n",
    "\n",
    "    clf = best_estimator\n",
    "\n",
    "    X = pd.concat([X_train, X_test])\n",
    "    y = pd.concat([y_train, y_test])\n",
    "\n",
    "    y_pred = print_results(clf, X_train, X_test, y_train, y_test, False)\n",
    "\n",
    "    force_qualify_8_teams(y_pred, y_test, confIDs, False)\n",
    "\n",
    "    scores = cross_validation(clf, X, y, False)\n",
    "    cross_val_scores.append(scores)\n",
    "\n",
    "plt.boxplot(cross_val_scores, labels=['1', '2', '3', '4', '5', '6', '7', '8', '9'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
