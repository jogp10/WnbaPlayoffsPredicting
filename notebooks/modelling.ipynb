{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843b0d3d",
   "metadata": {},
   "source": [
    "# Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ac332",
   "metadata": {},
   "source": [
    "## Get the data from the previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ebe47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.492929300Z",
     "start_time": "2023-11-05T23:33:50.115650400Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fd8e1",
   "metadata": {},
   "source": [
    "##### Get the data from the previous notebook, as csv files, and load them into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_players_df = pd.read_csv('../prep_data/dfs/awards_players_df.csv')\n",
    "coaches_df = pd.read_csv('../prep_data/dfs/coaches_df.csv')\n",
    "players_df = pd.read_csv('../prep_data/dfs/players_df.csv')\n",
    "players_teams_df = pd.read_csv('../prep_data/dfs/players_teams_df.csv')\n",
    "series_post_df = pd.read_csv('../prep_data/dfs/series_post_df.csv')\n",
    "teams_post_df = pd.read_csv('../prep_data/dfs/teams_post_df.csv')\n",
    "teams_df = pd.read_csv('../prep_data/prepared_dataset.csv')\n",
    "\n",
    "# Make a dictionary with all the dataframes\n",
    "dfs = {'awards_players_df': awards_players_df, 'coaches_df': coaches_df, 'players_df': players_df, 'players_teams_df': players_teams_df, 'series_post_df': series_post_df, 'teams_df': teams_df, 'teams_post_df': teams_post_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d82f15",
   "metadata": {},
   "source": [
    "### Get the data from competition year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Credentials\n",
    "with open(\"../config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "host = config[\"db_host\"]\n",
    "user = config[\"db_user\"]\n",
    "password = config[\"db_password\"]\n",
    "database = config[\"db_database\"]\n",
    "schema = config[\"db_11_schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "def execute(query):\n",
    "    cursor.execute(query)\n",
    "    connection.commit()\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def fetch(query):\n",
    "    cursor.execute(query)\n",
    "    return cursor.fetchall()\n",
    "\n",
    "SELECT = \"SELECT * FROM \" + schema + \".\" # + table_name \n",
    "INSERT = \"INSERT INTO \" + schema + \".\" # + table_name + \" VALUES \" + values\n",
    "UPDATE = \"UPDATE \" + schema + \".\" # + table_name + \" SET \" + column_name + \" = \" + value\n",
    "DELETE = \"DELETE FROM \" + schema + \".\"  # + table_name + \" WHERE \" + column_name + \" = \" + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76495aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "coaches = fetch(SELECT + \"coaches\") # all coaches who've managed the teams during the time period,\n",
    "players_teams = fetch(SELECT + \"players_teams\") # performance of each player for each team they played,\n",
    "teams = fetch(SELECT + \"teams\") # performance of the teams for each season,\n",
    "\n",
    "players_teams_11_df = pd.DataFrame(players_teams, columns=['playerID', 'year', 'stint', 'tmID', 'lgID'])\n",
    "teams_11_df = pd.DataFrame(teams, columns=['year', 'lgID', 'tmID', 'franchID', 'confID', 'name', 'arena', 'playoff'])\n",
    "coaches_11_df = pd.DataFrame(coaches, columns=['coachID', 'year', 'tmID', 'lgID', 'stint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3c020",
   "metadata": {},
   "source": [
    "##### Make some preprocessing to year 11 data to make it compatible with the other years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea81321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the DataFrame\n",
    "def franchise_mapping(teams_df, dfs):\n",
    "    team_franchise_mapping = {}\n",
    "    franchise_team_mapping = {}  # dictionary for reverse mapping\n",
    "    \n",
    "    for _, row in teams_df.iterrows():\n",
    "        # Extract team and franchise IDs from the current row\n",
    "        team_id = row['tmID']\n",
    "        franchise_id = row['franchID']\n",
    "\n",
    "        # Check if the team ID is not already in the mapping dictionary\n",
    "        if team_id not in team_franchise_mapping:\n",
    "            # Add the team ID and its corresponding franchise ID to the mapping\n",
    "            team_franchise_mapping[team_id] = franchise_id\n",
    "            franchise_team_mapping[franchise_id] = team_id\n",
    "\n",
    "    # Now, team_franchise_mapping contains the mapping between team IDs and franchise IDs\n",
    "    print(team_franchise_mapping)\n",
    "\n",
    "    for _, df in dfs.items():\n",
    "        # Check if 'tmID' is a column in the current DataFrame\n",
    "        if 'tmID' in df.columns:\n",
    "            # Replace team IDs with franchise IDs using the mapping\n",
    "            df['tmID'] = df['tmID'].map(team_franchise_mapping)\n",
    "        if 'franchID' in df.columns:\n",
    "            # Drop the 'franchID' column\n",
    "            df.drop(columns=['franchID'], inplace=True)\n",
    "\n",
    "    return team_franchise_mapping, franchise_team_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac638e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = ['confID', 'playoff']\n",
    "\n",
    "for col in binary_columns:\n",
    "    teams_11_df[col] = teams_11_df[col].replace('EA', 0)\n",
    "    teams_11_df[col] = teams_11_df[col].replace('WE', 1)\n",
    "    teams_11_df[col] = teams_11_df[col].replace('N', 0)\n",
    "    teams_11_df[col] = teams_11_df[col].replace('Y',1)\n",
    "\n",
    "team_franchise_map, franchise_team_map = franchise_mapping(teams_11_df, {'teams': teams_11_df, 'players_teams': players_teams_11_df, 'coaches':coaches_11_df})\n",
    "\n",
    "\n",
    "players_teams_11_df = players_teams_11_df.reindex(columns=players_teams_df.columns)\n",
    "players_teams_df = pd.concat([players_teams_11_df, players_teams_df])\n",
    "dfs['players_teams_df'] = players_teams_df\n",
    "\n",
    "teams_11_df = teams_11_df.reindex(columns=teams_df.columns)\n",
    "teams_df = pd.concat([teams_11_df, teams_df])\n",
    "dfs['teams_df'] = teams_df\n",
    "\n",
    "coaches_11_df = coaches_11_df.reindex(columns=coaches_df.columns)\n",
    "coaches_df = pd.concat([coaches_11_df, coaches_df])\n",
    "dfs['coaches_df'] = coaches_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d931a226",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "We will normalize the data, so models likes knn, linear regression and logistic regression, that assume that the data is normally distributed, can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the numeric columns in players_teams_df verify if they follow gaussian distribution\n",
    "def gaussian_distribution(df, output = False):\n",
    "    gaussian_columns = []\n",
    "    non_gaussian_columns = []\n",
    "\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        # Check if the column has 'NA' values\n",
    "        if 'NA' in df[col].unique():\n",
    "            continue\n",
    "\n",
    "        # Evaluate if the column follows a Gaussian distribution\n",
    "        _, p_value = stats.shapiro(df[col].dropna())  # Drop 'NA' values for the test\n",
    "        alpha = 0.05\n",
    "        if p_value > alpha:\n",
    "            gaussian_columns.append(col)\n",
    "            if output:\n",
    "                print('{} follows a Gaussian distribution'.format(col))\n",
    "        else:\n",
    "            non_gaussian_columns.append(col)\n",
    "            if output:\n",
    "                print('{} does not follow a Gaussian distribution'.format(col))\n",
    "\n",
    "    return gaussian_columns, non_gaussian_columns\n",
    "\n",
    "gaussian_distribution(dfs['teams_df'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(X_train, X_test):\n",
    "    X_train_normalized = X_train.copy()\n",
    "    X_test_normalized = X_test.copy()\n",
    "    \n",
    "    gaussian_columns, non_gaussian_columns = gaussian_distribution(X_train_normalized)\n",
    "    \n",
    "    # For the columns that follow a Gaussian distribution, apply StandardScaler; otherwise, apply MinMaxScaler\n",
    "    for col in non_gaussian_columns:\n",
    "        if col not in ['year', 'confID', 'playoff']:\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_normalized[col] = scaler.fit_transform(X_train_normalized[[col]])\n",
    "            X_test_normalized[col] = scaler.transform(X_test_normalized[[col]])\n",
    "\n",
    "    for col in gaussian_columns:\n",
    "        if col not in ['year', 'confID', 'playoff']:\n",
    "            scaler = StandardScaler()\n",
    "            X_train_normalized[col] = scaler.fit_transform(X_train_normalized[[col]])\n",
    "            X_test_normalized[col] = scaler.transform(X_test_normalized[[col]])\n",
    "    \n",
    "    return X_train_normalized, X_test_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676924c",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1623f13",
   "metadata": {},
   "source": [
    "### Constants and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f84cbb",
   "metadata": {},
   "source": [
    "Define some constants that we will be going to use across the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAMES = ['No Playoffs', 'Playoffs']\n",
    "N_FOLDS_CV = 10\n",
    "PREDICTION_YEAR = 10 # Will be changed to 11 later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c69d2",
   "metadata": {},
   "source": [
    "##### Create reusable functions that will be useful across the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b45d2c",
   "metadata": {},
   "source": [
    "- `print_confusion_matrix`:\n",
    "Prints a confusion matrix as a heatmap, based on the provided true and predicted values.\n",
    "\n",
    "- `print_results`:\n",
    "Fits the model, predicts values, and outputs both the confusion matrix and accuracy score.\n",
    "\n",
    "- `cross_validation`:\n",
    "Conducts cross-validation on the model using the provided data.\n",
    "\n",
    "- `force_qualify_8_teams`:\n",
    "As we know from the WNBA rules, there are 8 teams that qualify for the playoffs. This function will be used to force the qualification of 8 tams, 4 from each conference, based on the predicted probabilities and true values.\n",
    "\n",
    "- `plot_learning_curve`:\n",
    "Generates a learning curve plot for a given model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d27304e9ece233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.683928100Z",
     "start_time": "2023-11-05T23:33:50.164392900Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(cm, target_names):\n",
    "    print(cm)\n",
    "    df_cm = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "    sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "def print_results(clf, X_train, X_test, y_train, y_test, refit=True, output=True):\n",
    "    # Fit the classifier on the training data\n",
    "    if refit:\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the classifier\n",
    "    if output:\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Create a confusion matrix to evaluate the model and print with labels\n",
    "    if output:\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print_confusion_matrix(cm, TARGET_NAMES)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def cross_validation(clf, X, y, output=True):\n",
    "    cv = StratifiedKFold(n_splits=N_FOLDS_CV, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Display the cross-validation results\n",
    "    if output:\n",
    "        print(\"Cross-Validation Results:\")\n",
    "        print(\"Mean Accuracy: {:.2f}%\".format(scores.mean() * 100))\n",
    "        print(\"Standard Deviation: {:.2f}\".format(scores.std()))\n",
    "\n",
    "    return scores\n",
    "    \n",
    "def force_qualify_8_teams(clf, X_test, y_test, output=True):\n",
    "    y_pred_probs = clf.predict_proba(X_test)[:, 1]  # Get the probabilities for each team\n",
    "\n",
    "    # Combine predicted probabilities with conference information\n",
    "    pred_df = pd.DataFrame({'Probability': y_pred_probs, 'ConfID': X_test['confID']})\n",
    "\n",
    "    # Select the top 4 teams from each conference based on predicted probabilities\n",
    "    top_teams_indices = pred_df.groupby('ConfID')['Probability'].nlargest(4).index.get_level_values(1)\n",
    "\n",
    "    # Convert the labels to numeric format (0 for 'No Playoff', 1 for 'Playoff')\n",
    "    y_pred_numeric = [1 if i in top_teams_indices else 0 for i in y_test.index]\n",
    "\n",
    "    # Calculate the accuracy of the modified predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred_numeric)\n",
    "    if output:\n",
    "        print(\"Modified Accuracy (Top 4 Teams from Each Conference as Playoff):\", accuracy)\n",
    "\n",
    "    # Create a confusion matrix to evaluate the model and print with labels\n",
    "    if output:\n",
    "        cm = confusion_matrix(y_test, y_pred_numeric)\n",
    "        print_confusion_matrix(cm, TARGET_NAMES)\n",
    "\n",
    "    return y_pred_numeric\n",
    "\n",
    "def plot_learning_curve(clf, title, X, y, ylim=None, cv=10, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Loss\") \n",
    "    train_sizes, train_losses, test_losses = learning_curve(\n",
    "        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')  # Use neg_mean_squared_error as a loss\n",
    "    train_losses_mean = -np.mean(train_losses, axis=1)  # Reverse the sign for loss\n",
    "    train_losses_std = np.std(train_losses, axis=1)\n",
    "    test_losses_mean = -np.mean(test_losses, axis=1)  # Reverse the sign for loss\n",
    "    test_losses_std = np.std(test_losses, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_losses_mean - train_losses_std,\n",
    "                     train_losses_mean + train_losses_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_losses_mean - test_losses_std,\n",
    "                     test_losses_mean + test_losses_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_losses_mean, 'o-', color=\"r\",\n",
    "             label=\"Training loss\")\n",
    "    plt.plot(train_sizes, test_losses_mean, 'o-', color=\"g\",\n",
    "             label=\"Validation loss\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f045b",
   "metadata": {},
   "source": [
    "## With current year info prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df3c68",
   "metadata": {},
   "source": [
    "Run a simple model on the teams table to predict if a team will make it to the playoffs or not. Let's see how it behaves.\n",
    "\n",
    "The main objective of this chapter is to find which is the model that performs the best, given the perfect data (the data from the year to be predicted).\n",
    "\n",
    "We will only run simple models with the data of current year, because we expected them to perform very well as they are trained with the data that already tells us which teams will make it to the playoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e9886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.736022300Z",
     "start_time": "2023-11-05T23:33:50.172181100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train years and test year (year 10)\n",
    "teams_train_df = teams_df[teams_df['year'] < PREDICTION_YEAR]\n",
    "teams_test_df = teams_df[teams_df['year'] == PREDICTION_YEAR]\n",
    "\n",
    "cross_val_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c03a8a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.795542700Z",
     "start_time": "2023-11-05T23:33:50.192253800Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = teams_train_df['playoff']\n",
    "y_test = teams_test_df['playoff']\n",
    "X_train = teams_train_df.drop(['playoff', 'year', 'tmID'], axis=1)\n",
    "X_test = teams_test_df.drop(['playoff', 'year', 'tmID'], axis=1)\n",
    "\n",
    "# Normalize the data as KNN, Neural Networks, SVM, ... expect the data to be normalized\n",
    "X_train_normalized, X_test_normalized = normalize_df(X_train, X_test)\n",
    "\n",
    "X_normalized = pd.concat([X_train_normalized, X_test_normalized])\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a52fe1f",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "y_pred = print_results(clf, X_train, X_test, y_train, y_test)\n",
    "\n",
    "plot_learning_curve(clf, 'Random Forest Learning Curve', X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb9e60",
   "metadata": {},
   "source": [
    "As we expected, the model performs very well, with a 100% accuracy. From the learning curve we can see that the model is a good model, as the training and validation losses are low and are very close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2eafc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.954082900Z",
     "start_time": "2023-11-05T23:33:50.493928300Z"
    }
   },
   "outputs": [],
   "source": [
    "force_qualify_8_teams(clf, X_test, y_test)\n",
    "scores = cross_validation(clf, X, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78cf75",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691eef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "y_pred = print_results(knn_classifier, X_train_normalized, X_test_normalized, y_train, y_test)\n",
    "\n",
    "y_pred = force_qualify_8_teams(knn_classifier, X_test_normalized, y_test)\n",
    "\n",
    "scores = cross_validation(knn_classifier, X_normalized, y)\n",
    "cross_val_scores.append(scores)\n",
    "\n",
    "plot_learning_curve(knn_classifier, 'KNN Learning Curve', X_normalized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b5e7d",
   "metadata": {},
   "source": [
    "KNN didn't perform as well as Random Forest, but it still performed very well, with a 91% accuracy. From the learning curve we can see that the model is underfitting, as the losses are quite significant and the training and validation losses are far from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21f5297",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440ebe3",
   "metadata": {},
   "source": [
    "Using the current year information, the best model at their best. This permitted as to see which models will have a better performance with the data from the previous years, when we don't have the current year information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bf7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(cross_val_scores, labels=['Random Forest', 'KNN'])\n",
    "plt.title('Cross-Validation Scores')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484494c0",
   "metadata": {},
   "source": [
    "The Random Forest Classifier seems to be performing better than the other classifiers.\n",
    "\n",
    "In the next chapter we will explore using a rolling window to use past years for the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2556d35",
   "metadata": {},
   "source": [
    "## Rolling window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b0cbb",
   "metadata": {},
   "source": [
    "### Rolling window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efd014",
   "metadata": {},
   "source": [
    "We will use a rolling window to predict the next season.\n",
    "In this dataset, 3 years are combined, with more weight given to the most recent year, and combining that historic data with the target variable (playoff) from the target year (the n+1 year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a78b5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:50.980071300Z",
     "start_time": "2023-11-05T23:33:50.733839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the rolling window size (number of past years to consider)\n",
    "ROLLING_WIN_SIZE = 3  # You can adjust this value\n",
    "PREDICTION_YEAR = 11\n",
    "\n",
    "def create_rolling_window_dataset(df, rolling_win_size):\n",
    "    '''This function creates a list of dataframes, where each \n",
    "    dataframe contains the data from the previous n years.\n",
    "    It creates an entry for each year that the team has played.'''\n",
    "\n",
    "    df_rolling = []\n",
    "    for year in range(rolling_win_size + 1, PREDICTION_YEAR + 1):\n",
    "        df_year = df[df['year'] == year]\n",
    "        df_prev_years = df[(df['year'] < year) & (df['year'] >= year - rolling_win_size)]\n",
    "\n",
    "        # Take the data into a df for each team\n",
    "        for team in df_year['tmID'].unique():\n",
    "            # Take the data from the previous years for the same team\n",
    "            df_team_prev_years = df_prev_years[df_prev_years['tmID'] == team]\n",
    "\n",
    "            # Add the dataframe to the list\n",
    "            df_rolling.append(df_team_prev_years)\n",
    "\n",
    "    return df_rolling\n",
    "\n",
    "def generate_weights(window_size):\n",
    "    weights = np.linspace(0.1, 1, window_size)\n",
    "    weights /= weights.sum()  # Normalize weights to ensure they sum to 1\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_df():\n",
    "    # Create the rolling window datasets\n",
    "    rolling_window_historic = create_rolling_window_dataset(teams_df,  ROLLING_WIN_SIZE)\n",
    "\n",
    "    weighted_features = teams_df.drop(['year', 'tmID', 'confID', 'playoff'], axis=1).select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    # Create a new dataframe to store the rolling window data\n",
    "    rolling_data = pd.DataFrame(columns=teams_df.columns)\n",
    "\n",
    "    for window in rolling_window_historic:\n",
    "\n",
    "        #Create a new empty line for the rolling_data dataframe\n",
    "        rolling_data_line = pd.DataFrame(columns=teams_df.columns)\n",
    "\n",
    "        if len(window) != 0:\n",
    "            #1. Get the weights for the weighted average\n",
    "            weights = generate_weights(len(window))\n",
    "\n",
    "            #2. Get the weighted average of the weighted features\n",
    "            #Note: The weighted average is calculated as follows:\n",
    "            #weighted average = sum of (weight * feature value) / sum of weights\n",
    "            #For example, if the weights are [0.2, 0.3, 0.5] and the feature values are [10, 20, 30], then the weighted average is:\n",
    "            #weighted average = (0.2 * 10 + 0.3 * 20 + 0.5 * 30) / (0.2 + 0.3 + 0.5) = 24\n",
    "            for feature in weighted_features:\n",
    "                weighted_average = np.average(window[feature].values, weights=weights)\n",
    "                rolling_data_line[feature] = [weighted_average]\n",
    "\n",
    "            #3. Join that data with the data from the following year (the year we want to predict)\n",
    "            #Note: The data from the following year will be used as the target label, get next year from the teams_df dataframe\n",
    "            next_year = window['year'].max() + 1\n",
    "            next_year_data = teams_df[(teams_df['tmID'] == window['tmID'].values[0]) & (teams_df['year'] == next_year)]\n",
    "\n",
    "        #Add year, confID, tmID, and playoff columns\n",
    "        try:\n",
    "            rolling_data_line['year'] = next_year\n",
    "            rolling_data_line['tmID'] = window['tmID'].values[0]\n",
    "            rolling_data_line['confID'] = next_year_data['confID'].values[0]\n",
    "            rolling_data_line['playoff'] = next_year_data['playoff'].values[0]\n",
    "        except:\n",
    "            #If there is no data for the next year, skip this line\n",
    "            #print('No data for next year')\n",
    "            continue\n",
    "        \n",
    "        #4. Concat the data to the rolling_data dataframe\n",
    "        rolling_data = pd.concat([rolling_data, rolling_data_line])\n",
    "\n",
    "    rolling_data = rolling_data.reset_index(drop=True)\n",
    "\n",
    "    return rolling_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_data = rolling_window_df()\n",
    "rolling_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e0253",
   "metadata": {},
   "source": [
    "### Random Forest with Rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafde03cbdc1cd88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T23:33:52.246618Z",
     "start_time": "2023-11-05T23:33:50.748554200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = rolling_data[rolling_data['year'] < PREDICTION_YEAR]['playoff'].astype(int)\n",
    "y_test = rolling_data[rolling_data['year'] == PREDICTION_YEAR]['playoff'].astype(int)\n",
    "X_train = rolling_data[rolling_data['year'] < PREDICTION_YEAR].drop(['playoff', 'year', 'tmID'], axis=1).dropna(axis=1)\n",
    "X_test = rolling_data[rolling_data['year'] == PREDICTION_YEAR].drop(['playoff', 'year', 'tmID'], axis=1).dropna(axis=1)\n",
    "X_train_normalized, X_test_normalized = normalize_df(X_train, X_test)\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "X_normalized = pd.concat([X_train_normalized, X_test_normalized])\n",
    "y = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca70677",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "y_pred = print_results(clf, X_train, X_test, y_train, y_test)\n",
    "force_qualify_8_teams(clf, X_test, y_test)\n",
    "\n",
    "scores = cross_validation(clf, X, y)\n",
    "\n",
    "plot_learning_curve(clf, 'Random Forest Learning Curve', X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9011ec4",
   "metadata": {},
   "source": [
    "The learning curve tells us that the model with 100 estimators does not look like a good model, as the training and validation losses are far from each other and the losses are quite significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158bde5",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a40358",
   "metadata": {},
   "source": [
    "Now we will perform some grid search on some models with the rolling window data, to see if we can improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a096888",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339c779",
   "metadata": {},
   "source": [
    "Now, we will try to find what are the best parameters for the Random Forest, KNN and more models, using Grid Search, and tune this models to see if we can improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d29aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use grid search to find the best parameters\n",
    "cross_val_scores = []\n",
    "best_clf = {}\n",
    "grid_search_params = {}\n",
    "grid_search_models = {}\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators': [5, 10, 50],\n",
    "    'max_depth': [3, 5, 11], \n",
    "    'max_features': ['sqrt', 'log2', None], \n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10] \n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "grid_search_models['Random Forest'] = clf\n",
    "grid_search_params['Random Forest'] = parameters\n",
    "\n",
    "grid_search = GridSearchCV(clf, parameters, cv=10, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "\n",
    "clf = grid_search.best_estimator_\n",
    "best_clf['Random Forest'] = (grid_search.best_score_, grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbb230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best parameters to train the model\n",
    "y_pred = print_results(clf, X_train, X_test, y_train, y_test)\n",
    "force_qualify_8_teams(clf, X_test, y_test)\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "plot_learning_curve(clf, 'Random Forest best estimator Learning Curve', X, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63ae80",
   "metadata": {},
   "source": [
    "The curve is showing that the model improved a little bit, as the validation losses are closer now to the training losses, but the losses are still quite significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5cb18",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use grid search to find the best parameters\n",
    "parameters = {'n_neighbors': [3, 5, 7, 9, 11, 13], 'weights': ['uniform', 'distance']}\n",
    "clf = KNeighborsClassifier()\n",
    "grid_search_models['KNN'] = clf\n",
    "grid_search_params['KNN'] = parameters\n",
    "\n",
    "grid_search = GridSearchCV(clf, parameters, cv=10, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_normalized, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "\n",
    "clf = grid_search.best_estimator_\n",
    "best_clf['KNN'] = (grid_search.best_score_, grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best parameters to train the model\n",
    "y_pred = print_results(clf, X_train_normalized, X_test_normalized, y_train, y_test)\n",
    "force_qualify_8_teams(clf, X_test_normalized, y_test)\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "plot_learning_curve(clf, 'KNN best estimator Learning Curve', X_normalized, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abb9b0",
   "metadata": {},
   "source": [
    "On this model, the learning curve is having too high values of loss, even though the training and validation losses are close to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29ed9d",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"learning_rate\": [0.01, 0.1],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 2),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 2),\n",
    "    \"max_depth\":[3,8],\n",
    "    \"max_features\":[\"log2\",None],\n",
    "    \"subsample\":[0.8, 1.0],\n",
    "    \"n_estimators\":[100]\n",
    "}\n",
    "clf = GradientBoostingClassifier()\n",
    "grid_search_models['Gradient Boosting'] = clf\n",
    "grid_search_params['Gradient Boosting'] = parameters\n",
    "\n",
    "grid_search = GridSearchCV(clf, parameters, cv=10, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "\n",
    "clf = grid_search.best_estimator_\n",
    "best_clf['Gradient Boosting'] = (grid_search.best_score_, grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30431da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best parameters to train the model\n",
    "y_pred = print_results(clf, X_train, X_test, y_train, y_test)\n",
    "force_qualify_8_teams(clf, X_test, y_test)\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "plot_learning_curve(clf, 'Gradient Boosting best estimator Learning Curve', X, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de694288",
   "metadata": {},
   "source": [
    "Gradient Boost can be better than Random Forest, but it is more prone to overfitting and it is more difficult to tune. At this moment Random Forest is performing better than Gradient Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62201ef",
   "metadata": {},
   "source": [
    "### XBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['confID'] = X_train['confID'].astype('bool')\n",
    "X_test['confID'] = X_test['confID'].astype('bool')\n",
    "X = pd.concat([X_train, X_test])\n",
    "\n",
    "parameters = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'n_estimators': [100, 200]\n",
    "}\n",
    "clf = xgb.XGBClassifier()\n",
    "grid_search_models['XGBoost'] = clf\n",
    "grid_search_params['XGBoost'] = parameters\n",
    "\n",
    "grid_search = GridSearchCV(clf, parameters, cv=10, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "\n",
    "clf = grid_search.best_estimator_\n",
    "best_clf['XGBoost'] = (grid_search.best_score_, grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best parameters to train the model\n",
    "y_pred = print_results(clf, X_train, X_test, y_train, y_test)\n",
    "force_qualify_8_teams(clf, X_test, y_test)\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "cross_val_scores.append(scores)\n",
    "plot_learning_curve(clf, 'XGBoost best estimator Learning Curve', X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca0a1e",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'C': [0.01, 0.1, 1], \n",
    "    'gamma': ['scale'], \n",
    "    'kernel': ['linear'],\n",
    "    'class_weight': ['balanced'],\n",
    "    'probability': [True]\n",
    "}\n",
    "clf = svm.SVC()\n",
    "grid_search_models['SVM'] = clf\n",
    "grid_search_params['SVM'] = parameters\n",
    "\n",
    "grid_search = GridSearchCV(clf, parameters, cv=10, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_normalized, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "clf = grid_search.best_estimator_\n",
    "best_clf['SVM'] = (grid_search.best_score_, grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best parameters to train the model\n",
    "y_pred = print_results(clf, X_train_normalized, X_test_normalized, y_train, y_test)\n",
    "force_qualify_8_teams(clf, X_test_normalized, y_test)\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "plot_learning_curve(clf, 'SVM best estimator Learning Curve', X_normalized, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820721e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'C': [0.01, 0.1, 1],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'class_weight': ['balanced'],\n",
    "    'solver': ['liblinear'],\n",
    "    'max_iter': [100],\n",
    "}\n",
    "\n",
    "clf = LogisticRegression()\n",
    "grid_search_models['LogisticRegression'] = clf\n",
    "grid_search_params['LogisticRegression'] = parameters\n",
    "\n",
    "grid_search = GridSearchCV(clf, parameters, cv=10, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_normalized, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "print(\"Best Estimator:\", grid_search.best_estimator_)\n",
    "\n",
    "clf = grid_search.best_estimator_\n",
    "best_clf['LogisticRegression'] = (grid_search.best_score_, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e02a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best parameters to train the model\n",
    "y_pred = print_results(clf, X_train_normalized, X_test_normalized, y_train, y_test)\n",
    "force_qualify_8_teams(clf, X_test_normalized, y_test)\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "plot_learning_curve(clf, 'Logistic Regression Learning Curve', X_normalized, y)\n",
    "cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f505ae25",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779219b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the cross validation scores\n",
    "plt.boxplot(cross_val_scores, labels=best_clf.keys())\n",
    "plt.title('Cross-Validation Scores')\n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ab287",
   "metadata": {},
   "source": [
    "After getting our best classifier, we will try different numbers of ROLLING WINDOW to see if we can improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b07adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store the best score and estimator\n",
    "best_score = -1\n",
    "best_estimator = None\n",
    "best_clf_name = None\n",
    "\n",
    "# Iterate over the best_clf dictionary\n",
    "for clf_name, (score, estimator) in best_clf.items():\n",
    "    # If this classifier's score is higher than the current best score, update the best score and estimator\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_estimator = estimator\n",
    "        best_clf_name = clf_name\n",
    "\n",
    "print(f\"The best classifier is {best_clf_name} with a score of {best_score}\")\n",
    "\n",
    "cross_val_scores = []\n",
    "best_year = 0\n",
    "best_score = -1\n",
    "for i in range(1, 10):\n",
    "    ROLLING_WIN_SIZE = i\n",
    "    rolling_data = rolling_window_df()\n",
    "\n",
    "    clf = best_estimator\n",
    "\n",
    "    X = pd.concat([X_train, X_test])\n",
    "    y = pd.concat([y_train, y_test])\n",
    "\n",
    "    y_pred = print_results(clf, X_train, X_test, y_train, y_test, output=False)\n",
    "\n",
    "    scores = cross_validation(clf, X, y, False)\n",
    "    cross_val_scores.append(scores)\n",
    "\n",
    "    if scores.mean() > best_score:\n",
    "        best_score = scores.mean()\n",
    "        best_year = i\n",
    "\n",
    "plt.boxplot(cross_val_scores, labels=['1', '2', '3', '4', '5', '6', '7', '8', '9'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f9f1b3",
   "metadata": {},
   "source": [
    "Now we will use the rolling window we created on the previous notebook, we will base our statistics on the last years of the players that currrently play on the roster of the team.\n",
    "As we could ee from before, when creating the rolling window data the more years we use in the past, less their weight. So we can barely notice any difference between using 3 or 10 years of data into account. So we will use 3 years of data, as it is the most recent data for this next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b071ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "cross_val_scores = []\n",
    "\n",
    "rolling_data = pd.read_csv('../prep_data/rolling_window/data_with_' + str(3) +'_years_in_the_past.csv')\n",
    "\n",
    "y_train = rolling_data[rolling_data['year'] < PREDICTION_YEAR]['playoff'].astype(int)\n",
    "y_test = rolling_data[rolling_data['year'] == PREDICTION_YEAR]['playoff'].astype(int)\n",
    "confIDs = rolling_data[rolling_data['year'] == PREDICTION_YEAR]['confID']\n",
    "tmIDs = rolling_data[rolling_data['year'] == PREDICTION_YEAR]['tmID']\n",
    "X_train = rolling_data[rolling_data['year'] < PREDICTION_YEAR].drop(['playoff', 'year', 'tmID'], axis=1).dropna(axis=1)\n",
    "X_test = rolling_data[rolling_data['year'] == PREDICTION_YEAR].drop(['playoff', 'year', 'tmID'], axis=1).dropna(axis=1)\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])\n",
    "\n",
    "for model in grid_search_models.keys():\n",
    "    if model in ['KNN', 'SVM', 'XGBoost']:\n",
    "        # Normalize the data as KNN, SVM, ... expect the data to be normalized\n",
    "        X_train_model, X_test_model = normalize_df(X_train, X_test)\n",
    "        X_model = pd.concat([X_train_model, X_test_model])\n",
    "    else:\n",
    "        X_train_model, X_test_model = X_train, X_test\n",
    "        X_model = X\n",
    "        \n",
    "    clf = grid_search_models[model]\n",
    "    parameters = grid_search_params[model]\n",
    "\n",
    "    labels.append(model + '_w' + str(3) + 'years')\n",
    "    \n",
    "    clf = best_clf[model][1]\n",
    "    clf.fit(X_train_model, y_train)\n",
    "    \n",
    "\n",
    "    best_clf[model] = (best_clf[model][0], best_clf[model][1], [X_train_model, X_test_model, y_train, y_test], 3)\n",
    "\n",
    "    y_pred = print_results(clf, X_train_model, X_test_model, y_train, y_test, output=False)\n",
    "\n",
    "    scores = cross_validation(clf, X_model, y, False)\n",
    "    cross_val_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2706b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 10))\n",
    "plt.boxplot(cross_val_scores, labels=labels)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title('Rolling data size: ' +  str(3))\n",
    "plt.show()\n",
    "\n",
    "best_score = -1\n",
    "best_classifier = None\n",
    "for best_name, best in best_clf.items():\n",
    "    print(best_name + '( score: ' + str(best[0]) + ', with ' + str(3) + ' years behind, estimator:' + str(best[1]) + ')')\n",
    "\n",
    "    if best[0] > best_score:\n",
    "        best_score = best[0]\n",
    "        best_classifier = best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2aebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_data = pd.read_csv('../prep_data/rolling_window/data_with_' + str(3) +'_years_in_the_past.csv')\n",
    "\n",
    "with open('results.txt', 'w') as f:\n",
    "    y_pred_without_8 = print_results(best_classifier[1], best_classifier[2][0], best_classifier[2][1], best_classifier[2][2], best_classifier[2][3])\n",
    "    y_pred = force_qualify_8_teams(best_classifier[1], best_classifier[2][1], y_test)\n",
    "\n",
    "    X = pd.concat([best_classifier[2][0], best_classifier[2][1]])\n",
    "    y = pd.concat([best_classifier[2][2], best_classifier[2][3]])\n",
    "    # Convert y_pred_numeric to a DataFrame and reset the index\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=[\"playoff_predict\"])\n",
    "    y_pred_without_8_df = pd.DataFrame(y_pred_without_8, columns=[\"playoff_predict_without_8\"])\n",
    "\n",
    "    # Filter competition_data for the prediction year\n",
    "    competition_data = competition_data[rolling_data['year'] == PREDICTION_YEAR][['tmID', 'year', 'confID', 'playoff']].sort_values('tmID', ascending=True)\n",
    "\n",
    "    # Merge competition_data with y_pred_df\n",
    "    result = competition_data.reset_index(drop=True).join(y_pred_df)\n",
    "    result = result.reset_index(drop=True).join(y_pred_without_8_df).sort_values(['confID', 'tmID'], ascending=True)\n",
    "\n",
    "    result['playoff'] = result['playoff'].map({1: 'Y', 0: 'N'})\n",
    "    result['tmID'] = result['tmID'].map(franchise_team_map)\n",
    "\n",
    "    result_subset = result[['tmID', 'playoff', 'playoff_predict', 'playoff_predict_without_8']]\n",
    "    result_subset = result_subset.sort_values('tmID')\n",
    "\n",
    "    # Write the result to a CSV file\n",
    "    result_subset.to_csv('output_data/output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('output_data/best.pkl', 'wb') as model_file:\n",
    "#     pickle.dump(best_classifier[1], model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
