{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843b0d3d",
   "metadata": {},
   "source": [
    "# Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ac332",
   "metadata": {},
   "source": [
    "## Get the data from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ebe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "USE_MERGED_DATA = True\n",
    "\n",
    "#Get the data from the previous notebook, as csv files, and load them into dataframes\n",
    "awards_players_df = pd.read_csv('../prep_data/awards_players_df.csv')\n",
    "coaches_df = pd.read_csv('../prep_data/coaches_df.csv')\n",
    "players_df = pd.read_csv('../prep_data/players_df.csv')\n",
    "players_teams_df = pd.read_csv('../prep_data/players_teams_df.csv')\n",
    "series_post_df = pd.read_csv('../prep_data/series_post_df.csv')\n",
    "# teams_df = pd.read_csv('../prep_data/teams_df_with_average_attributes_of_players.csv.csv') isto dá 100% nalgumas cenas, mas muito menos no rolling window, deve ser um overfit manhoso, estranho\n",
    "# a soma dos 3 melhores de cada atributo dá essencialmente o mesmo que a média e normalizar o teams_df original não fez diferença nenhuma\n",
    "if USE_MERGED_DATA:\n",
    "    teams_df = pd.read_csv('../prep_data/teams_df_with_sum_of_3_best_attributes_of_players.csv')\n",
    "else:\n",
    "    teams_df = pd.read_csv('../prep_data/teams_df.csv')\n",
    "teams_post_df = pd.read_csv('../prep_data/teams_post_df.csv')\n",
    "\n",
    "#make a dictionary with all the dataframes\n",
    "dfs = {'awards_players_df': awards_players_df, 'coaches_df': coaches_df, 'players_df': players_df, 'players_teams_df': players_teams_df, 'series_post_df': series_post_df, 'teams_df': teams_df, 'teams_post_df': teams_post_df}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print all the dataframes\n",
    "for key in dfs:\n",
    "    print(key)\n",
    "    print(dfs[key].head())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676924c",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(cm, target_names):\n",
    "    df_cm = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "    sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ],
   "id": "f7d27304e9ece233"
  },
  {
   "cell_type": "markdown",
   "id": "0a0f045b",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df3c68",
   "metadata": {},
   "source": [
    "Run a simple model on the teams table to predict if a team will make it to the playoffs or not. Let's see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teams_df\n",
    "\n",
    "# split into train years and test year (year 10)\n",
    "teams_train_df = teams_df[teams_df['year'] < 10]\n",
    "teams_test_df = teams_df[teams_df['year'] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b9c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see all null values in teams_df\n",
    "teams_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c03a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a tree decision classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Split the data into features and target label\n",
    "target_names = ['No Playoffs', 'Playoffs']\n",
    "\n",
    "y_train = teams_train_df['playoff']\n",
    "y_test = teams_test_df['playoff']\n",
    "X_train = teams_train_df.drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "X_test = teams_test_df.drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "\n",
    "if USE_MERGED_DATA:\n",
    "    X_train = X_train.drop(['W_post', 'L_post'], axis=1)\n",
    "    X_test = X_test.drop(['W_post', 'L_post'], axis=1)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create a confusion matrix to evaluate the model and print with labels\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print_confusion_matrix(cm, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_teams_indices = y_pred.argsort()[-8:][::-1]  # Get the indices of the top 8 teams\n",
    "\n",
    "# Convert the labels to numeric format (0 for 'No Playoff', 1 for 'Playoff')\n",
    "y_pred_numeric = [1 if i in top_teams_indices else 0 for i in range(len(y_pred))]\n",
    "\n",
    "# Calculate the accuracy of the modified predictions\n",
    "accuracy = accuracy_score(y_test, y_pred_numeric)\n",
    "print(\"Modified Accuracy (Top 8 Teams as Playoff):\", accuracy)\n",
    "\n",
    "# Create a confusion matrix to evaluate the model and print with labels\n",
    "cm = confusion_matrix(y_test, y_pred_numeric)\n",
    "print_confusion_matrix(cm, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2556d35",
   "metadata": {},
   "source": [
    "## Rolling window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efd014",
   "metadata": {},
   "source": [
    "We will use a rolling window to predict the next season. We will use the last 3 seasons to predict the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a78b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rolling window size (number of past years to consider)\n",
    "ROLLING_WIN_SIZE = 3  # You can adjust this value\n",
    "PREDICTION_YEAR = 10\n",
    "\n",
    "historical_data_df = teams_df[teams_df['year'] < PREDICTION_YEAR]\n",
    "\n",
    "def create_rolling_window_dataset(df, window_size):\n",
    "    rolling_window_data = []\n",
    "    for tmID, group in df.groupby('tmID'):\n",
    "        group = group.sort_values(by='year')\n",
    "        for i in range(len(group) - window_size + 1):\n",
    "            window = group.iloc[i:i + window_size]\n",
    "            rolling_window_data.append(window)\n",
    "    return rolling_window_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into features and target label\n",
    "target_names = ['No Playoff', 'Playoff']\n",
    "\n",
    "# Create the rolling window datasets\n",
    "rolling_window_historic = create_rolling_window_dataset(historical_data_df,  ROLLING_WIN_SIZE)\n",
    "\n",
    "# Define weights (you can adjust these weights as needed)\n",
    "weights = [0.2, 0.3, 0.5]  # Assign higher weight to the last year\n",
    "\n",
    "#Columns to apply the weighted moving average to\n",
    "# weighted_features = ['o_fta', 'o_3pm', 'o_oreb', 'o_asts', 'o_stl', 'o_to', 'o_blk', 'd_3pa', 'd_oreb', 'd_dreb', 'd_asts', 'd_stl', 'd_to', 'd_blk', 'attend']\n",
    "\n",
    "weighted_features = ['o_fta', 'o_3pm', 'o_oreb', 'o_asts', 'o_stl', 'o_to', 'o_blk', 'd_3pa', 'd_oreb', 'd_dreb', 'd_asts', 'd_stl', 'd_to', 'd_blk', 'attend']\n",
    "\n",
    "if USE_MERGED_DATA:\n",
    "    weighted_features += ['num_seasons', 'height', 'weight', 'stint', 'GP', 'threeAttempted', 'dq', 'PostoRebounds', 'PostBlocks', 'PostthreeAttempted', 'PostDQ', 'FG%', 'FT%', 'Avg_Assists_Per_Game', 'Avg_Blocks_Per_Game', 'Avg_Steals_Per_Game', 'Points_Ratio', 'POSITION_METRIC']\n",
    "\n",
    "# Create a new dataframe to store the rolling window data\n",
    "rolling_data = pd.DataFrame(columns=teams_df.columns)\n",
    "\n",
    "for window in rolling_window_historic:\n",
    "    #For each window, we will add a line to the rolling_data dataframe, as follows:\n",
    "    #1. Create a new empty line for the rolling_data dataframe\n",
    "    rolling_data_line = pd.DataFrame(columns=teams_df.columns)\n",
    "\n",
    "    #2. Get the weighted average of the weighted features\n",
    "    #Note: The weighted average is calculated as follows:\n",
    "    #weighted average = sum of (weight * feature value) / sum of weights\n",
    "    #For example, if the weights are [0.2, 0.3, 0.5] and the feature values are [10, 20, 30], then the weighted average is:\n",
    "    #weighted average = (0.2 * 10 + 0.3 * 20 + 0.5 * 30) / (0.2 + 0.3 + 0.5) = 24\n",
    "    for feature in weighted_features:\n",
    "        weighted_average = np.average(window[feature].values, weights=weights)\n",
    "        rolling_data_line[feature] = [weighted_average]\n",
    "\n",
    "    #3. Join that data with the data from the following year (the year we want to predict)\n",
    "    #Note: The data from the following year will be used as the target label, get next year from the teams_df dataframe\n",
    "    next_year = window['year'].max() + 1\n",
    "    next_year_data = teams_df[(teams_df['tmID'] == window['tmID'].values[0]) & (teams_df['year'] == next_year)]\n",
    "\n",
    "    #Add year, confID, tmID, and playoff columns\n",
    "    try:\n",
    "        rolling_data_line['year'] = next_year\n",
    "        rolling_data_line['tmID'] = window['tmID'].values[0]\n",
    "        rolling_data_line['confID'] = next_year_data['confID'].values[0]\n",
    "        rolling_data_line['playoff'] = next_year_data['playoff'].values[0]\n",
    "    except:\n",
    "        #If there is no data for the next year, skip this line\n",
    "        #print('No data for next year')\n",
    "        continue\n",
    "    \n",
    "    #4. Concat the data to the rolling_data dataframe\n",
    "    rolling_data = pd.concat([rolling_data, rolling_data_line])\n",
    "\n",
    "rolling_data = rolling_data.reset_index(drop=True)\n",
    "\n",
    "# Split the data into features and target label\n",
    "# Test data will be the 10th year\n",
    "target_names = ['No Playoff', 'Playoff']\n",
    "\n",
    "y_train = rolling_data[rolling_data['year'] < 10]['playoff'].astype(int)\n",
    "y_test = rolling_data[rolling_data['year'] == 10]['playoff'].astype(int)\n",
    "X_train = rolling_data[rolling_data['year'] < 10].drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "X_test = rolling_data[rolling_data['year'] == 10].drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "\n",
    "if USE_MERGED_DATA:\n",
    "    X_train = X_train.drop(['W_post', 'L_post'], axis=1)\n",
    "    X_test = X_test.drop(['W_post', 'L_post'], axis=1)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "# clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create a confusion matrix to evaluate the model and print with labels\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print_confusion_matrix(cm, target_names)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cafde03cbdc1cd88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_teams_indices = y_pred.argsort()[-8:][::-1]  # Get the indices of the top 8 teams\n",
    "\n",
    "# Convert the labels to numeric format (0 for 'No Playoff', 1 for 'Playoff')\n",
    "y_pred_numeric = [1 if i in top_teams_indices else 0 for i in range(len(y_pred))]\n",
    "\n",
    "# Calculate the accuracy of the modified predictions\n",
    "accuracy = accuracy_score(y_test, y_pred_numeric)\n",
    "print(\"Modified Accuracy (Top 8 Teams as Playoff):\", accuracy)\n",
    "\n",
    "# Create a confusion matrix to evaluate the model and print with labels\n",
    "cm = confusion_matrix(y_test, y_pred_numeric)\n",
    "print_confusion_matrix(cm, target_names)"
   ],
   "id": "40efb52aa11b8e48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46e42b11b49afbde"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into features and target label\n",
    "\n",
    "target_names = ['No Playoff', 'Playoff']\n",
    "\n",
    "y_train = teams_train_df['playoff']\n",
    "y_test = teams_test_df['playoff']\n",
    "X_train = teams_train_df.drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "X_test = teams_test_df.drop(['playoff', 'year', 'tmID', 'firstRound', 'semis', 'finals'], axis=1)\n",
    "\n",
    "if USE_MERGED_DATA:\n",
    "    X_train = X_train.drop(['W_post', 'L_post'], axis=1)\n",
    "    X_test = X_test.drop(['W_post', 'L_post'], axis=1)\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = random_forest_classifier.predict(X_test)\n",
    "\n",
    "feature_importances = pd.DataFrame(random_forest_classifier.feature_importances_, index = X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "display(feature_importances)\n",
    "\n",
    "# print the used Decision Tree\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_tree(random_forest_classifier.estimators_[0], feature_names=X_train.columns, filled=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95af36e88216eb81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create a confusion matrix to evaluate the model and print with labels\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print_confusion_matrix(cm, target_names)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d9f10fcf039dba3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# try knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2907368686d702d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Fiz isto aqui porque não encontrei onde foram criados os csvs de /prep_data e queria garantir que estava usar a mesma coisa\n",
    "# SPOILER: fez 0 diferença\n",
    "\"\"\"\n",
    "for col in dfs['teams_df'].select_dtypes(include=np.number).columns:\n",
    "    if col not in ['year', 'confID', 'playoff', 'firstRound', 'semis', 'finals']:\n",
    "        aux = (dfs['teams_df'][col] - dfs['teams_df'][col].min()) / (dfs['teams_df'][col].max() - dfs['teams_df'][col].min())\n",
    "        dfs['teams_df'][col] = aux.round(3)\n",
    "# store in csv in prep_data with 2 appended to the name\n",
    "dfs['teams_df'].to_csv('../prep_data/teams_df2.csv', index=False)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f202f413765e9c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a5f6a2cae942ce5f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
