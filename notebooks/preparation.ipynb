{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a474e2",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb42898",
   "metadata": {},
   "source": [
    "## Database Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aeeee5",
   "metadata": {},
   "source": [
    "We used a free service to host our database. The Database is in PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Credentials\n",
    "with open(\"../config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "host = config[\"db_host\"]\n",
    "user = config[\"db_user\"]\n",
    "password = config[\"db_password\"]\n",
    "database = config[\"db_database\"]\n",
    "schema = config[\"db_schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "def execute(query):\n",
    "    cursor.execute(query)\n",
    "    connection.commit()\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def fetch(query):\n",
    "    cursor.execute(query)\n",
    "    return cursor.fetchall()\n",
    "\n",
    "SELECT = \"SELECT * FROM \" + schema + \".\" # + table_name \n",
    "INSERT = \"INSERT INTO \" + schema + \".\" # + table_name + \" VALUES \" + values\n",
    "UPDATE = \"UPDATE \" + schema + \".\" # + table_name + \" SET \" + column_name + \" = \" + value\n",
    "DELETE = \"DELETE FROM \" + schema + \".\"  # + table_name + \" WHERE \" + column_name + \" = \" + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44416216",
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_players = fetch(SELECT + \"awards_players\") # awards and prizes received by players across 10 seasons,\n",
    "coaches = fetch(SELECT + \"coaches\") # all coaches who've managed the teams during the time period,\n",
    "players = fetch(SELECT + \"players\") # details of all players,\n",
    "players_teams = fetch(SELECT + \"players_teams\") # performance of each player for each team they played,\n",
    "series_post = fetch(SELECT + \"series_post\") # series' results,\n",
    "teams = fetch(SELECT + \"teams\") # performance of the teams for each season,\n",
    "teams_post = fetch(SELECT + \"teams_post\") # results of each team at the post-season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4454347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data in a dataframe\n",
    "awards_players_df = pd.DataFrame(awards_players, columns=['playerID', 'award', 'year', 'lgID'])\n",
    "coaches_df = pd.DataFrame(coaches, columns=['coachID', 'year', 'tmID', 'lgID', 'stint', 'won', 'lost', 'post_wins', 'post_losses'])\n",
    "players_df = pd.DataFrame(players, columns=['bioID', 'pos', 'firstseason', 'lastseason', 'height', 'weight', 'college', 'collegeOther', 'birthDate', 'deathDate'])\n",
    "players_teams_df = pd.DataFrame(players_teams, columns=['playerID', 'year', 'stint', 'tmID', 'lgID', 'GP', 'GS', 'minutes', 'points', 'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade', 'dq', 'PostGP', 'PostGS', 'PostMinutes', 'PostPoints', 'PostoRebounds', 'PostdRebounds', 'PostRebounds', 'PostAssists', 'PostSteals', 'PostBlocks', 'PostTurnovers', 'PostPF', 'PostfgAttempted', 'PostfgMade', 'PostftAttempted', 'PostftMade', 'PostthreeAttempted', 'PostthreeMade', 'PostDQ'])\n",
    "series_post_df = pd.DataFrame(series_post, columns=['year', 'round', 'series', 'tmIDWinner', 'lgIDWinner', 'tmIDLoser', 'lgIDLoser', 'W', 'L'])\n",
    "teams_df = pd.DataFrame(teams, columns=['year', 'lgID', 'tmID', 'franchID', 'confID', 'divID', 'rank', 'playoff', 'seeded', 'firstRound', 'semis', 'finals', 'name', 'o_fgm', 'o_fga', 'o_ftm', 'o_fta', 'o_3pm', 'o_3pa', 'o_oreb', 'o_dreb', 'o_reb', 'o_asts', 'o_pf', 'o_stl', 'o_to', 'o_blk', 'o_pts', 'd_fgm', 'd_fga', 'd_ftm', 'd_fta', 'd_3pm', 'd_3pa', 'd_oreb', 'd_dreb', 'd_reb', 'd_asts', 'd_pf', 'd_stl', 'd_to', 'd_blk', 'd_pts', 'tmORB', 'tmDRB', 'tmTRB', 'opptmORB', 'opptmDRB', 'opptmTRB', 'won', 'lost', 'GP', 'homeW', 'homeL', 'awayW', 'awayL', 'confW', 'confL', 'min', 'attend', 'arena'])\n",
    "teams_post_df = pd.DataFrame(teams_post, columns=['year', 'tmID', 'lgID', 'W', 'L'])\n",
    "\n",
    "#make a dictionary with all the dataframes\n",
    "dfs = {'awards_players_df': awards_players_df, 'coaches_df': coaches_df, 'players_df': players_df, 'players_teams_df': players_teams_df, 'series_post_df': series_post_df, 'teams_df': teams_df, 'teams_post_df': teams_post_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17109c4",
   "metadata": {},
   "source": [
    "So with that we end our understanding phase.\n",
    "Our main takeaways are:\n",
    "- There are dead players in the players table. We should take that into account when doing the analysis.\n",
    "- There are players that have not played any season of the seasons given. We should take that into account when doing the analysis. There are 338 players that have not played any season.\n",
    "- There are no Null entries (although there values that are simply an empty string)\n",
    "- There are some columns with the DataType \"object\", most of them being strings.\n",
    "- There are binary objects (like confID and playoff, in the 'teams' table, with the values \"Y\" or \"N\") that could be substituted by a binary, as well as ternary objects (like the firstRound, semis and finals in the 'teams' table, with the values \"W\", \"L\" or \"\") that could also be transformed.\n",
    "- There are players with no position and no college assigned (\"\").\n",
    "- There are players with no date of birth in the record (0000-00-00).\n",
    "- There is the need to do null value uniformization, as there are some columns with empty strings, others with default 0 values and other values that represent null.\n",
    "- The height and weight variables have default 0 values and should be treated as null values.\n",
    "- The number of games played by each team differs (there may be teams that are no longer playing), so we can't compare the number of wins and losses directly. Win percentage should be used.\n",
    "- In terms of win percentage, it seems like a competitive league, with more than half of the teams having a win percentage of 50% or more, taking advantage of the worst teams. There is also just one team below 40% of wins.\n",
    "- There are teams that are no longer playing.\n",
    "- There are a lot of highly correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601ae40",
   "metadata": {},
   "source": [
    "## Preparing the data for the model\n",
    "\n",
    "In this notebook, we will prepare the data for the model. Having done the understanding in the [previous notebook](understanding.ipynb), we will now prepare the data for the model. From the understanding we came to the following conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642023b",
   "metadata": {},
   "source": [
    "So with that we end our understanding phase.\n",
    "Our main takeaways are:\n",
    "- There are dead players in the players table. We should take that into account when doing the analysis.\n",
    "- There are players that have not played any season of the seasons given. We should take that into account when doing the analysis. There are 338 players that have not played any season.\n",
    "- There are no Null entries (although there values that are simply an empty string)\n",
    "- There are some columns with the DataType \"object\", most of them being strings.\n",
    "- There are binary objects (like confID and playoff, in the 'teams' table, with the values \"Y\" or \"N\") that could be substituted by a binary, as well as ternary objects (like the firstRound, semis and finals in the 'teams' table, with the values \"W\", \"L\" or \"\") that could also be transformed.\n",
    "- There are players with no position and no college assigned (\"\").\n",
    "- There are players with no date of birth in the record (0000-00-00).\n",
    "- There is the need to do null value uniformization, as there are some columns with empty strings, others with default 0 values and other values that represent null.\n",
    "- The height and weight variables have default 0 values and should be treated as null values.\n",
    "- The number of games played by each team differs (there may be teams that are no longer playing), so we can't compare the number of wins and losses directly. Win percentage should be used.\n",
    "- In terms of win percentage, it seems like a competitive league, with more than half of the teams having a win percentage of 50% or more, taking advantage of the worst teams. There is also just one team below 40% of wins.\n",
    "- There are teams that are no longer playing.\n",
    "- There are a lot of highly correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90834ca",
   "metadata": {},
   "source": [
    "After considering our takeaways, we will now prepare the data for the model. We will do the following:\n",
    "- Remove the players that have not played any season, and, if a player died, remove the seasons after the death.\n",
    "- Transform the binary objects into binary values.\n",
    "- Transform the ternary objects into binary values. (where the third value is a null value - after the null uniformization these are considered as binary objects too)\n",
    "- Null uniformization: transform the empty strings and default 0 values into null values.\n",
    "- Analysis null values: analyze the null values and decide what to do with them.\n",
    "- Calculate win percentage for each team and add it to the teams table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2ce92",
   "metadata": {},
   "source": [
    "We begin by excluding columns that consistently have identical values since they do not contribute any valuable information to the model. However, we will retain the 'first season' and 'last season' of a player, as we intend to populate them with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a375c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns whose values are always the same\n",
    "for df in dfs:\n",
    "    for col in dfs[df].columns:\n",
    "        if len(dfs[df][col].unique()) == 1 and col not in ['firstseason', 'lastseason'] :\n",
    "            print(df, col)\n",
    "            dfs[df].drop(col, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7679878",
   "metadata": {},
   "source": [
    "### Null uniformization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9448bff",
   "metadata": {},
   "source": [
    "We identified the following columns that have null values, but are not identified as such:\n",
    "- players: height, weight, birthDate, position, college, deathDate\n",
    "- teams: firstRound, semis, finals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941273cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If date == 00-00-00, replace with null (birthDate and deathDate)\n",
    "\n",
    "dfs[\"players_df\"][\"birthDate\"] = dfs[\"players_df\"][\"birthDate\"].replace('00-00-00', None)\n",
    "dfs[\"players_df\"][\"birthDate\"] = dfs[\"players_df\"][\"birthDate\"].replace('0000-00-00', None)\n",
    "dfs[\"players_df\"][\"deathDate\"] = dfs[\"players_df\"][\"deathDate\"].replace('00-00-00', None)\n",
    "dfs[\"players_df\"][\"deathDate\"] = dfs[\"players_df\"][\"deathDate\"].replace('0000-00-00', None)\n",
    "\n",
    "# If value == 0, replace with median (height, weight)\n",
    "\n",
    "dfs[\"players_df\"][\"height\"].fillna(dfs[\"players_df\"][\"height\"].mean(), inplace=True)\n",
    "dfs[\"players_df\"][\"weight\"].fillna(dfs[\"players_df\"][\"weight\"].mean(), inplace=True)\n",
    "\n",
    "# If value == \"\", replace with null (college, collegeOther, firstRound, semis, finals)\n",
    "\n",
    "dfs[\"players_df\"][\"college\"] = dfs[\"players_df\"][\"college\"].replace('', None)\n",
    "dfs[\"players_df\"][\"collegeOther\"] = dfs[\"players_df\"][\"collegeOther\"].replace('', None)\n",
    "dfs[\"teams_df\"][\"firstRound\"] = dfs[\"teams_df\"][\"firstRound\"].replace('', None)\n",
    "dfs[\"teams_df\"][\"semis\"] = dfs[\"teams_df\"][\"semis\"].replace('', None)\n",
    "dfs[\"teams_df\"][\"finals\"] = dfs[\"teams_df\"][\"finals\"].replace('', None)\n",
    "\n",
    "dfs[\"players_df\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907828e",
   "metadata": {},
   "source": [
    "### Remove the players that have not played any season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#players that have not played in the last 10 years\n",
    "players_not_played = fetch(\"SELECT p.bioid FROM wnba.players p WHERE p.bioid not in (select pt.playerid  from wnba.players_teams pt)\")\n",
    "print(\"Number of players that haven't played: \" + \n",
    "      str(len(players_not_played)))\n",
    "\n",
    "players_not_played_df = pd.DataFrame(players_not_played, columns=['bioID'])\n",
    "\n",
    "players_not_played_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861dba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the number of players that have not played in the last 10 years, and the lenght of the 3 dataframes that contain the playerID\n",
    "print(\"Number of players that have not played: \", len(players_not_played_df['bioID'].unique()))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Number of values in the players_team_df: \", len(dfs['players_teams_df']['playerID'].unique()))\n",
    "print(\"Number of values in the awards_players_df: \", len(dfs['awards_players_df']['playerID'].unique()))\n",
    "print(\"Number of values in the players_df: \", len(dfs['players_df']['bioID'].unique()))\n",
    "\n",
    "#Remove the players that have not played in the last 10 years\n",
    "for df in dfs:\n",
    "    if(df == 'players_teams_df' or df == 'awards_players_df'):\n",
    "        dfs[df] = dfs[df][~dfs[df]['playerID'].isin(players_not_played_df['bioID'])]\n",
    "    if(df == 'players_df'):\n",
    "        dfs[df] = dfs[df][~dfs[df]['bioID'].isin(players_not_played_df['bioID'])]\n",
    "\n",
    "#Print the number of players that have not played in the last 10 years, and the lenght of the 3 dataframes that contain the playerID\n",
    "print('\\n')\n",
    "print(\"Number of values in the players_team_df: \", len(dfs['players_teams_df']['playerID'].unique()))\n",
    "print(\"Number of values in the awards_players_df: \", len(dfs['awards_players_df']['playerID'].unique()))\n",
    "print(\"Number of values in the players_df: \", len(dfs['players_df']['bioID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Death date as only 4 players have it and it is not relevant for the analysis, as we are not interested in the death date of the players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[\"players_df\"] = dfs[\"players_df\"].drop('deathDate', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasnform birthDate into birthYear, so the model can use it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'birthDate' to datetime if it's not already\n",
    "dfs[\"players_df\"]['birthDate'] = pd.to_datetime(dfs[\"players_df\"]['birthDate'])\n",
    "\n",
    "# Create a new 'birthYear' column\n",
    "dfs[\"players_df\"]['birthYear'] = dfs[\"players_df\"]['birthDate'].dt.year\n",
    "dfs[\"players_df\"] = dfs[\"players_df\"].drop('birthDate', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82108b56",
   "metadata": {},
   "source": [
    "### Populate first and last seasons of a player in the wnba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac6861",
   "metadata": {},
   "source": [
    "As we mentioned before we will populate first and last season of a player in the wnba. We will do this by looking at the seasons table and finding the first and last season of a player. We will then populate the first and last season of a player in the players table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the players_teams_df by 'playerID' to find the first and last seasons.\n",
    "first_seasons = dfs['players_teams_df'].groupby('playerID')['year'].min()\n",
    "last_seasons = dfs['players_teams_df'].groupby('playerID')['year'].max()\n",
    "\n",
    "# Use .loc to set the values in players_df without the warning.\n",
    "dfs['players_df'].loc[:, 'firstseason'] = dfs['players_df']['bioID'].map(first_seasons)\n",
    "dfs['players_df'].loc[:, 'lastseason'] = dfs['players_df']['bioID'].map(last_seasons)\n",
    "\n",
    "print(dfs['players_df'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304bca3",
   "metadata": {},
   "source": [
    "### Transform the binary objects into binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the binary columns from all the dataframes\n",
    "binary_columns = []\n",
    "for df in dfs:\n",
    "    binary_columns = binary_columns + [(df, list(dfs[df].columns[dfs[df].nunique() == 2]))]\n",
    "\n",
    "#Print the binary columns uniques values\n",
    "for i in binary_columns:\n",
    "    if(len(i[1]) < 0):\n",
    "        continue\n",
    "\n",
    "    for j in i[1]:\n",
    "        print(\"-------\")\n",
    "        print(i[0], j)\n",
    "        print(dfs[i[0]][j].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0f9b3",
   "metadata": {},
   "source": [
    "GP is the number of games played and should also not be converted to binary, as we will need this value to calculate the win percentage. (it is only binary because seasons have been of 32 games or 34 games). The W value in the series_post represents the number of wins a team winned in the playoffs. All the playoffs games are in the best of 3 or 5, so the winning team wins 2 or 3 games.\n",
    "The other binary values are binary and should be converted to binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the binary columns to 0 and 1 (confID, playoff, firstRound, semis, finals)\n",
    "\n",
    "columns = ['firstRound', 'semis', 'finals']\n",
    "for col in columns:\n",
    "    dfs[\"teams_df\"][col].fillna('NQ', inplace=True)\n",
    "\n",
    "binary_columns = [\"confID\", \"playoff\", \"firstRound\", \"semis\", \"finals\"]\n",
    "\n",
    "for col in binary_columns:\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('EA', 0)\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('WE', 1)\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('L', 0)\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('W', 1)\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('NQ', -1)\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('N', 0)\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('Y',1)\n",
    "\n",
    "#change the type of the column to int\n",
    "for col in binary_columns:\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].astype(\"Int64\")\n",
    "\n",
    "dfs[\"teams_df\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093bcae",
   "metadata": {},
   "source": [
    "### Calculate win percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01b188f",
   "metadata": {},
   "source": [
    "For each team, we want to calculate the following:\n",
    "- Win percentage\n",
    "- Loss percentage\n",
    "- Wins at home percentage\n",
    "- Losses at home percentage\n",
    "- Wins away percentage\n",
    "- Losses away percentage\n",
    "- Conference wins percentage\n",
    "- Conference losses percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8cf7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate win percentage, loss percentage, wins at home percentage, losses at home percentage, wins away percentage, losses away percentage, wins at conference percentage, losses at conference percentage\n",
    "\n",
    "dfs[\"teams_df\"][\"win_percentage\"] = dfs[\"teams_df\"][\"won\"] / (dfs[\"teams_df\"][\"won\"] + dfs[\"teams_df\"][\"lost\"])\n",
    "dfs[\"teams_df\"][\"loss_percentage\"] = dfs[\"teams_df\"][\"lost\"] / (dfs[\"teams_df\"][\"won\"] + dfs[\"teams_df\"][\"lost\"])\n",
    "dfs[\"teams_df\"][\"home_win_percentage\"] = dfs[\"teams_df\"][\"homeW\"] / (dfs[\"teams_df\"][\"homeW\"] + dfs[\"teams_df\"][\"homeL\"])\n",
    "dfs[\"teams_df\"][\"home_loss_percentage\"] = dfs[\"teams_df\"][\"homeL\"] / (dfs[\"teams_df\"][\"homeW\"] + dfs[\"teams_df\"][\"homeL\"])\n",
    "dfs[\"teams_df\"][\"away_win_percentage\"] = dfs[\"teams_df\"][\"awayW\"] / (dfs[\"teams_df\"][\"awayW\"] + dfs[\"teams_df\"][\"awayL\"])\n",
    "dfs[\"teams_df\"][\"away_loss_percentage\"] = dfs[\"teams_df\"][\"awayL\"] / (dfs[\"teams_df\"][\"awayW\"] + dfs[\"teams_df\"][\"awayL\"])\n",
    "dfs[\"teams_df\"][\"conference_win_percentage\"] = dfs[\"teams_df\"][\"confW\"] / (dfs[\"teams_df\"][\"confW\"] + dfs[\"teams_df\"][\"confL\"])\n",
    "dfs[\"teams_df\"][\"conference_loss_percentage\"] = dfs[\"teams_df\"][\"confL\"] / (dfs[\"teams_df\"][\"confW\"] + dfs[\"teams_df\"][\"confL\"])\n",
    "\n",
    "#Drop the columns that are not needed anymore\n",
    "dfs[\"teams_df\"] = dfs[\"teams_df\"].drop(columns=['won', 'lost', 'homeW', 'homeL', 'awayW', 'awayL', 'confW', 'confL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e082e",
   "metadata": {},
   "source": [
    "## Data Preparation on players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef549740",
   "metadata": {},
   "source": [
    "### Position Uniformization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c8427",
   "metadata": {},
   "source": [
    "From the list below we can see that there are 7 different positions. We will uniformize the positions to the following:\n",
    "- Guard (G)\n",
    "- Forward (F)\n",
    "- Center (C)\n",
    "- Guard-Forward (G-F)\n",
    "- Forward-Center (F-C)\n",
    "\n",
    "But, as we can see from the distinct positions, we have 2 more positions that are not in the list above. These are:\n",
    "(C-F) and (F-G). We will uniformize these positions to the ones above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_positions = dfs['players_df']['pos'].unique()\n",
    "print(unique_positions)\n",
    "\n",
    "# Define specific_position_mapping\n",
    "specific_position_mapping = {\n",
    "    'F-G': 'G-F',\n",
    "    'C-F': 'F-C'\n",
    "}\n",
    "\n",
    "# Use .loc to update the 'pos' column in players_df\n",
    "dfs['players_df'].loc[:, 'pos'] = dfs['players_df'].loc[:, 'pos'].replace(specific_position_mapping)\n",
    "\n",
    "# Check the unique values after mapping\n",
    "print(\"After mapping\")\n",
    "unique_positions = dfs['players_df']['pos'].unique()\n",
    "print(unique_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d59c60",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded1942",
   "metadata": {},
   "source": [
    "We will now prepare the data for the players table. We will do the following:\n",
    "- Feature engineering: create a column with the number of seasons a player played in the wnba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['players_df'].loc[:, 'num_seasons'] = dfs['players_df']['lastseason'] - dfs['players_df']['firstseason'] + 1\n",
    "dfs['players_df'] = dfs['players_df'].drop(columns=['firstseason', 'lastseason'])\n",
    "\n",
    "dfs['players_df'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b28d9",
   "metadata": {},
   "source": [
    "- Total Points in the season\n",
    "- Total Rebounds in the season\n",
    "- Total Assists in the season\n",
    "- Total Steals in the season\n",
    "- Total Turnovers in the season\n",
    "- Total Goal Percentage in the season\n",
    "- Total Three Point Percentage in the season\n",
    "- Total Free Throw Percentage in the season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by 'playerID', 'year' and add the stats from the many stints, keep the last team played for in the season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'playerID' and 'year', sum the stats, and keep the last 'teamID'\n",
    "df_grouped = dfs[\"players_teams_df\"].groupby(['playerID', 'year']).agg({**{col: 'sum' for col in dfs[\"players_teams_df\"].columns if col not in ['playerID', 'year', 'tmID']}, **{'tmID': 'last'}}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9673ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[\"players_teams_df\"][\"total_points\"] = (\n",
    "    dfs[\"players_teams_df\"][\"points\"] + dfs[\"players_teams_df\"][\"PostPoints\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"total_rebounds\"] = (\n",
    "    dfs[\"players_teams_df\"][\"rebounds\"] + dfs[\"players_teams_df\"][\"PostRebounds\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"total_assists\"] = (\n",
    "    dfs[\"players_teams_df\"][\"assists\"] + dfs[\"players_teams_df\"][\"PostAssists\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"total_blocks\"] = (\n",
    "    dfs[\"players_teams_df\"][\"blocks\"] + dfs[\"players_teams_df\"][\"PostBlocks\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"total_steals\"] = (\n",
    "    dfs[\"players_teams_df\"][\"steals\"] + dfs[\"players_teams_df\"][\"PostSteals\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"total_turnovers\"] = (\n",
    "    dfs[\"players_teams_df\"][\"turnovers\"] + dfs[\"players_teams_df\"][\"PostTurnovers\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"FG%\"] = (\n",
    "    dfs[\"players_teams_df\"][\"fgMade\"] + dfs[\"players_teams_df\"][\"PostfgMade\"]\n",
    ") / (\n",
    "    dfs[\"players_teams_df\"][\"fgAttempted\"] + dfs[\"players_teams_df\"][\"PostfgAttempted\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"FT%\"] = (\n",
    "    dfs[\"players_teams_df\"][\"ftMade\"] + dfs[\"players_teams_df\"][\"PostftMade\"]\n",
    ") / (\n",
    "    dfs[\"players_teams_df\"][\"ftAttempted\"] + dfs[\"players_teams_df\"][\"PostftAttempted\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"FG%\"].fillna(0, inplace=True)\n",
    "dfs[\"players_teams_df\"][\"FT%\"].fillna(0, inplace=True)\n",
    "\n",
    "# Create a new 'award' column\n",
    "dfs[\"players_teams_df\"]['award'] = dfs[\"players_teams_df\"].set_index(['playerID', 'year']).index.isin(dfs[\"awards_players_df\"].groupby(['playerID', 'year']).any().index).astype(int)\n",
    "\n",
    "# Average Stats\n",
    "dfs[\"players_teams_df\"][\"Avg_Points_Per_Game\"] = (\n",
    "    dfs[\"players_teams_df\"][\"total_points\"] / dfs[\"players_teams_df\"][\"GP\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"Avg_Rebounds_Per_Game\"] = (\n",
    "    dfs[\"players_teams_df\"][\"total_rebounds\"] / dfs[\"players_teams_df\"][\"GP\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"Avg_Assists_Per_Game\"] = (\n",
    "    dfs[\"players_teams_df\"][\"total_assists\"] / dfs[\"players_teams_df\"][\"GP\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"Avg_Blocks_Per_Game\"] = (\n",
    "    dfs[\"players_teams_df\"][\"total_blocks\"] / dfs[\"players_teams_df\"][\"GP\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"Avg_Steals_Per_Game\"] = (\n",
    "    dfs[\"players_teams_df\"][\"total_steals\"] / dfs[\"players_teams_df\"][\"GP\"]\n",
    ")\n",
    "dfs[\"players_teams_df\"][\"Avg_Turnovers_Per_Game\"] = (\n",
    "    dfs[\"players_teams_df\"][\"total_turnovers\"] / dfs[\"players_teams_df\"][\"GP\"]\n",
    ")\n",
    "\n",
    "# Durability Ratio (Check if this is correct)\n",
    "dfs[\"players_teams_df\"][\"Usability\"] = (\n",
    "    dfs[\"players_teams_df\"][\"minutes\"]\n",
    ") / (dfs[\"players_teams_df\"][\"GP\"] * 40)\n",
    "\n",
    "# Get the player position from dfs['players']['pos'], match ['bioid'] to ['playerID'] in dfs['players_teams_df']\n",
    "position_mapping = dfs[\"players_df\"].set_index(\"bioID\")[\"pos\"]\n",
    "dfs[\"players_teams_df\"][\"pos\"] = dfs[\"players_teams_df\"][\"playerID\"].map(\n",
    "    position_mapping\n",
    ")\n",
    "\n",
    "\n",
    "# Define a dictionary of position-specific metrics\n",
    "position_metrics = {\n",
    "    \"G\": \"AST_TO_RATIO\",\n",
    "    \"F\": \"REBOUND_EFFICIENCY\",\n",
    "    \"G-F\": \"3P_SHOOTING_PERCENT\",\n",
    "    \"C\": \"BLOCK_EFFICIENCY\",\n",
    "    \"F-C\": \"SCORING_EFFICIENCY\",\n",
    "}\n",
    "\n",
    "# Calculate position-specific metrics and update the DataFrame\n",
    "for position, metric in position_metrics.items():\n",
    "    position_df = dfs[\"players_teams_df\"][dfs[\"players_teams_df\"][\"pos\"] == position]\n",
    "    dfs[\"players_teams_df\"][metric] = position_df.apply(\n",
    "        lambda row: row[\"assists\"] / row[\"turnovers\"]\n",
    "        if metric == \"AST_TO_RATIO\" and row[\"turnovers\"] != 0\n",
    "        else (row[\"oRebounds\"] + row[\"dRebounds\"]) / row[\"GP\"]\n",
    "        if metric == \"REBOUND_EFFICIENCY\" and row[\"GP\"] != 0\n",
    "        else row[\"threeMade\"] / row[\"threeAttempted\"]\n",
    "        if metric == \"3P_SHOOTING_PERCENT\" and row[\"threeAttempted\"] != 0\n",
    "        else row[\"blocks\"] / row[\"GP\"]\n",
    "        if metric == \"BLOCK_EFFICIENCY\" and row[\"GP\"] != 0\n",
    "        else row[\"points\"] / row[\"fgAttempted\"]\n",
    "        if metric == \"SCORING_EFFICIENCY\" and row[\"fgAttempted\"] != 0\n",
    "        else None,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Normalize the metric\n",
    "    scaler = MinMaxScaler()\n",
    "    dfs[\"players_teams_df\"][[metric]] = scaler.fit_transform(dfs[\"players_teams_df\"][[metric]])\n",
    "\n",
    "\n",
    "# Join the position-specific metrics to the one column in dfs['players_teams_df']\n",
    "def extract_first_non_null(row):\n",
    "    for column in list(position_metrics.values()):\n",
    "        if not pd.isnull(row[column]):\n",
    "            return row[column]\n",
    "    return None\n",
    "dfs[\"players_teams_df\"][\"POSITION_METRIC\"] = dfs[\"players_teams_df\"].apply(\n",
    "    extract_first_non_null, axis=1\n",
    ")\n",
    "dfs[\"players_teams_df\"].drop(columns=list(position_metrics.values()), inplace=True)\n",
    "\n",
    "\n",
    "nulls_in_position_metric = dfs[\"players_teams_df\"][\"POSITION_METRIC\"].isnull().sum()\n",
    "print(f\"Number of null values in POSITION_METRIC due to having 0 at position caracteristic: {nulls_in_position_metric}\")\n",
    "dfs['players_teams_df']['POSITION_METRIC'].fillna(0, inplace=True)\n",
    "\n",
    "# Get columns that start with 'Post'\n",
    "post_columns = dfs['players_teams_df'].filter(like='Post').columns\n",
    "# Drop these columns\n",
    "dfs['players_teams_df'] = dfs['players_teams_df'].drop(columns=post_columns)\n",
    "\n",
    "dfs[\"players_teams_df\"].columns\n",
    "dfs[\"players_teams_df\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f739094",
   "metadata": {},
   "source": [
    "### Check correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b23916",
   "metadata": {},
   "source": [
    "We will remove the most correlated variables, as they do not add any value to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CORRELATION = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column to indicate if the player went to the playoffs to look for correlations\n",
    "def went_to_playoff(df, dfs):\n",
    "    returned_df = df.copy()\n",
    "    \n",
    "    playoff_data = dfs['teams_df'][['tmID', 'year', 'playoff']]\n",
    "\n",
    "    returned_df = returned_df.merge(playoff_data, on=['tmID', 'year'], how='left')\n",
    "    \n",
    "    return returned_df\n",
    "\n",
    "def delete_most_correlated(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    correlation_matrix = df_copy.corr()\n",
    "\n",
    "    sorted_correlations = correlation_matrix.unstack().sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "    # Get the pairs of attributes with the highest correlation values\n",
    "    most_correlated_pairs = sorted_correlations[sorted_correlations > MAX_CORRELATION]\n",
    "    most_correlated_pairs = most_correlated_pairs[most_correlated_pairs < 1.0]\n",
    "\n",
    "\n",
    "    #delete repeated pairs (e.g. (a,b) and (b,a))\n",
    "    most_correlated_pairs = most_correlated_pairs[::2]\n",
    "    print(most_correlated_pairs)\n",
    "\n",
    "    # Drop the attributes with the highest correlation values\n",
    "    for pair in most_correlated_pairs.index:\n",
    "        if 'Usability' not in pair and 'year' not in pair:\n",
    "            if pair[0] in df_copy.columns:\n",
    "                df_copy.drop(pair[0], inplace=True, axis=1)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618cb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    print(df)\n",
    "    if 'players_teams_df' in df:\n",
    "        dfs[df] = went_to_playoff(dfs[df], dfs)\n",
    "\n",
    "    #select only the numerical columns\n",
    "    # new_df = delete_most_correlated(dfs[df].select_dtypes(include=np.number))\n",
    "    new_df = dfs[df].select_dtypes(include=np.number)\n",
    "    #merge new_df with the categorical columns\n",
    "    print(new_df)\n",
    "    dfs[df] = new_df.merge(dfs[df].select_dtypes(exclude=np.number), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42d618",
   "metadata": {},
   "source": [
    "### Remove categorical variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb7970",
   "metadata": {},
   "source": [
    "There are several categorical variables that we will remove from the model, as they do not add any value to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove categorical columns from team_df that are not needed \n",
    "#franchID, name, arena\n",
    "\n",
    "dfs['teams_df'] = dfs['teams_df'].drop(columns=['name', 'arena'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some teams changed their names, but they maintained the same Franchise ID. We will map the teams to their Franchise ID and replace the team ID with the Franchise ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "def franchise_mapping(teams_df, dfs):\n",
    "    team_franchise_mapping = {}\n",
    "    for index, row in teams_df.iterrows():\n",
    "        # Extract team and franchise IDs from the current row\n",
    "        team_id = row['tmID']\n",
    "        franchise_id = row['franchID']\n",
    "\n",
    "        # Check if the team ID is not already in the mapping dictionary\n",
    "        if team_id not in team_franchise_mapping:\n",
    "            # Add the team ID and its corresponding franchise ID to the mapping\n",
    "            team_franchise_mapping[team_id] = franchise_id\n",
    "\n",
    "    # Now, team_franchise_mapping contains the mapping between team IDs and franchise IDs\n",
    "    print(team_franchise_mapping)\n",
    "\n",
    "    for df_name, df in dfs.items():\n",
    "        # Check if 'tmID' is a column in the current DataFrame\n",
    "        if 'tmID' in df.columns:\n",
    "            # Replace team IDs with franchise IDs using the mapping\n",
    "            df['tmID'] = df['tmID'].map(team_franchise_mapping)\n",
    "        if 'franchID' in df.columns:\n",
    "            # Drop the 'franchID' column\n",
    "            df.drop(columns=['franchID'], inplace=True)\n",
    "        \n",
    "franchise_mapping(dfs['teams_df'], dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6be1c673254a25",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Normalization and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433b7372df252d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the numeric columns in players_teams_df verify if they follow gaussian distribution\n",
    "for col in dfs['teams_df'].select_dtypes(include=np.number).columns:\n",
    "    # Check if the column has 'NA' values\n",
    "    if 'NA' in dfs['teams_df'][col].unique():\n",
    "        print(f\"Skipping {col} due to 'NA' values\")\n",
    "        continue\n",
    "\n",
    "    # Evaluate if the column follows a Gaussian distribution\n",
    "    statistics, p_value = stats.shapiro(dfs['teams_df'][col].dropna())  # Drop 'NA' values for the test\n",
    "    alpha = 0.05\n",
    "    if p_value > alpha:\n",
    "        print('{} follows a Gaussian distribution'.format(col))\n",
    "    else:\n",
    "        print('{} does not follow a Gaussian distribution'.format(col))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc7a096c303016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are no Gaussian distributions in the numeric columns of the players_teams_df, so we will only apply linear normalizations between 0 and 1.\n",
    "But there are such cases in the teams_df. NOT YET NORMALIZED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca4d928702f98f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for the numeric columns in players_teams_df apply linear normalization between 0 and 1\n",
    "for col in dfs['teams_df'].select_dtypes(include=np.number).columns:\n",
    "    if col not in ['year', 'confID', 'playoff']:\n",
    "        aux = (dfs['teams_df'][col] - dfs['teams_df'][col].min()) / (dfs['teams_df'][col].max() - dfs['teams_df'][col].min())\n",
    "        dfs['teams_df'][col] = aux.round(3)\n",
    "\n",
    "# store in csv in prep_data with 2 appended to the name\n",
    "# dfs['teams_df'].to_csv('../prep_data/teams_df2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d1451",
   "metadata": {},
   "source": [
    "### Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ffae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the tables in a csv file, inside the prep_data folder\n",
    "for df in dfs:\n",
    "    dfs[df].to_csv('../prep_data/' + df + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b904cfca0c968389",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Merge information into a single table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e26b4153ef79e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Insights on merging tables:\n",
    "- teams with teams_post, some values are NaN when the team didn't participate in the playoffs\n",
    "- players with players_teams, some values are NaN when the player didn't participate in any team, remove such cases?\n",
    "- awards with players\n",
    "- number of trophies won by each team in each season\n",
    "\n",
    "- more ideas...\n",
    "- create a score for each player's performance in each season and then aggregate it to the team's performance in each season?\n",
    "- Is series_post relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815602a660f467d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "teams_post_df = dfs['teams_post_df'].rename(columns={'W': 'W_post', 'L': 'L_post'})\n",
    "dfs['teams_df'] = dfs['teams_df'].merge(teams_post_df, on=['tmID', 'year'], how='left')\n",
    "\n",
    "dfs['teams_df'].head()\n",
    "\n",
    "# to csv\n",
    "# dfs['teams_df'].to_csv('teams_df_merge_with_post.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4da2e391f2f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join players (bioID) with players_teams (playerID)\n",
    "players_df  = dfs['players_df'].rename(columns={'bioID': 'playerID'})\n",
    "players_teams_df = players_teams_df.merge(players_df, on=['playerID'], how='left')\n",
    "\n",
    "# to csv\n",
    "dfs['players_teams_df'].to_csv('../prep_data/players_df_merge_with_players_teams.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bea00beb86d65b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming you have already imported players_df and teams_df\n",
    "teams_df2 = dfs['teams_df'].copy()\n",
    "\n",
    "# Iterate over each column in players_df\n",
    "numeric_columns = dfs['players_teams_df'].select_dtypes(include=np.number).columns\n",
    "exclude_columns = {'year', 'tmID', 'lgID', 'playerID', 'firstseason', 'lastseason', 'playoff', 'num_seasons', 'stint', 'GP', 'threeMade', 'dq', 'PostBlocks', 'PostthreeMade', 'PostDQ'}\n",
    "numeric_columns = [col for col in numeric_columns if col not in exclude_columns]\n",
    "\n",
    "for col in numeric_columns:\n",
    "    # Calculate the weighted mean using 'minutes' as weights within the groupby operation\n",
    "    weighted_mean = dfs['players_teams_df'].groupby(['year', 'tmID', 'playoff'])[col].apply(lambda x: np.average(x, weights=dfs['players_teams_df'].loc[x.index, 'Usability'])).reset_index(name=col + '_weighted_mean')\n",
    "\n",
    "    # Merge the weighted mean into teams_df2\n",
    "    teams_df2 = teams_df2.merge(weighted_mean, on=['year', 'tmID', 'playoff'], how='left')\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "dfs['teams_df'] = teams_df2.copy()\n",
    "teams_df2.to_csv('../prep_data/teams_df2.csv', index=False)\n",
    "\n",
    "teams_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ac5909f313725",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #do similarly but now instead of the average of each player's performance, calculate the sum of the 3 best players' performance\n",
    "# teams_df4 = dfs['teams_df'].copy()\n",
    "# for col in players_teams_df:\n",
    "#     col_type = players_teams_df[col].dtype\n",
    "#     if col_type == 'float64' or col_type == 'int64' and col not in {'year', 'tmID', 'lgID', 'playerID', 'firstseason', 'lastseason', 'playoff', 'num_seasons', 'stint', 'GP', 'threeMade', 'dq', 'PostBlocks', 'PostthreeMade', 'PostDQ'}:\n",
    "#         # Group by ['year', 'tmID', 'playoff'], sum the 3 largest values for each group\n",
    "#         grouped = players_teams_df.groupby(['year', 'tmID', 'playoff'])[col].apply(lambda x: x.nlargest(3).sum())\n",
    "        \n",
    "#         # Rename the grouped column to indicate it represents the sum of the 3 best players' performance\n",
    "#         col_name_sum = f'{col}_sum_top3'\n",
    "#         grouped = grouped.reset_index().rename(columns={col: col_name_sum})\n",
    "        \n",
    "#         # Merge the sum into teams_df4 with a specified suffix\n",
    "#         teams_df4 = pd.merge(teams_df4, grouped, on=['year', 'tmID', 'playoff'], how='left', suffixes=('', f'_{col_name_sum}'))\n",
    "\n",
    "# # dfs['teams_df'] = teams_df4.copy()\n",
    "# # teams_df4.to_csv('../prep_data/teams_df4.csv', index=False)\n",
    "# teams_df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trophies won by players on each team\n",
    "teams_df3 = dfs['teams_df'].copy()\n",
    "\n",
    "# Filter players_teams_df to include only rows where 'won_award' is 1\n",
    "players_teams_df = dfs['players_teams_df'][dfs['players_teams_df']['award'] == 1]\n",
    "\n",
    "# Group by tmID and year and count the number of awards per team\n",
    "team_trophies_count = players_teams_df.groupby(['tmID', 'year']).size().reset_index(name='num_trophies')\n",
    "\n",
    "# Merge with teams_df3\n",
    "teams_df3 = teams_df3.merge(team_trophies_count, on=['tmID', 'year'], how='left')\n",
    "\n",
    "# Replace NaN values with 0\n",
    "teams_df3['num_trophies'] = teams_df3['num_trophies'].fillna(0)\n",
    "\n",
    "# Update dfs['teams_df'] and save to csv\n",
    "dfs['teams_df'] = teams_df3.copy()\n",
    "teams_df3.to_csv('../prep_data/prepared_dataset.csv', index=False)\n",
    "\n",
    "teams_df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve data from competition year before going to modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = config[\"db_11_schema\"]\n",
    "\n",
    "SELECT = \"SELECT * FROM \" + schema + \".\" # + table_name \n",
    "coaches = fetch(SELECT + \"coaches\") # all coaches who've managed the teams during the time period,\n",
    "players_teams = fetch(SELECT + \"players_teams\") # performance of each player for each team they played,\n",
    "teams = fetch(SELECT + \"teams\") # performance of the teams for each season,\n",
    "\n",
    "players_teams_11_df = pd.DataFrame(players_teams, columns=['playerID', 'year', 'stint', 'tmID', 'lgID'])\n",
    "teams_11_df = pd.DataFrame(teams, columns=['year', 'lgID', 'tmID', 'franchID', 'confID', 'name', 'arena', 'playoff'])\n",
    "coaches_11_df = pd.DataFrame(coaches, columns=['coachID', 'year', 'tmID', 'lgID', 'stint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_11_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = ['confID', 'playoff']\n",
    "\n",
    "for col in binary_columns:\n",
    "    teams_11_df[col] = teams_11_df[col].replace('EA', 0)\n",
    "    teams_11_df[col] = teams_11_df[col].replace('WE', 1)\n",
    "    teams_11_df[col] = teams_11_df[col].replace('N', 0)\n",
    "    teams_11_df[col] = teams_11_df[col].replace('Y',1)\n",
    "\n",
    "franchise_mapping(teams_11_df, {'teams': teams_11_df, 'players_teams': players_teams_11_df, 'coaches':coaches_11_df})\n",
    "\n",
    "players_teams_11_df = players_teams_11_df.reindex(columns=dfs[\"players_teams_df\"].columns)\n",
    "dfs[\"players_teams_df\"] = pd.concat([dfs[\"players_teams_df\"], players_teams_11_df])\n",
    "\n",
    "teams_11_df = teams_11_df.reindex(columns=dfs[\"teams_df\"].columns)\n",
    "dfs[\"teams_df\"] = pd.concat([dfs[\"teams_df\"], teams_11_df])\n",
    "\n",
    "coaches_11_df = coaches_11_df.reindex(columns=dfs[\"coaches_df\"].columns)\n",
    "dfs[\"coaches_df\"] = pd.concat([dfs[\"coaches_df\"], coaches_11_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[\"teams_df\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = 11\n",
    "num_previous_years = test_year - 1  # Number of previous years to consider\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def weighted_mean(arr):\n",
    "    num_years = len(arr)\n",
    "    weights = np.arange(1, num_years + 1 ) * 1.0 / 10\n",
    "    return np.sum(arr * weights) / np.sum(weights)\n",
    "\n",
    "def weighted_average(arr, weights):\n",
    "    if len(arr) != len(weights):\n",
    "        raise ValueError(\"Array and weights must have the same length\")\n",
    "    return np.sum(arr * weights) / np.sum(weights)\n",
    "\n",
    "for i in range(1, num_previous_years + 1):\n",
    "\n",
    "    # Sort the players_teams DataFrame by player ID and season\n",
    "    players_teams_sorted = dfs[\"players_teams_df\"].sort_values(by=['playerID', 'year'], ascending=True)\n",
    "    # players_teams_sorted.to_csv('../prep_data/rolling_window/debug.csv', index=False)\n",
    "\n",
    "    numeric_columns = players_teams_sorted.select_dtypes(include=np.number).columns.difference(['year', 'stint', 'playoff'])\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        players_teams_sorted[col] = (\n",
    "            players_teams_sorted.groupby('playerID')[col]\n",
    "            .rolling(window=i, min_periods=1, closed='left')\n",
    "            .apply(weighted_mean, raw=False)\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "\n",
    "    # Group by 'teamId' and 'year' and aggregate the cumulative statistics (Weighted average by time of play of previous years)\n",
    "    team_players_year_stats = (\n",
    "        players_teams_sorted.groupby(['tmID', 'year'])\n",
    "        .apply(lambda x: pd.Series({\n",
    "            col: weighted_average(x[col], x['Usability']) for col in numeric_columns\n",
    "        })).reset_index()\n",
    "    )\n",
    "\n",
    "    # Apply the same to team stats\n",
    "    teams_sorted = dfs[\"teams_df\"].sort_values(by=['tmID', 'year'], ascending=True)\n",
    "\n",
    "    numeric_columns = teams_sorted.select_dtypes(include=np.number).columns.difference(['year', 'confID', 'playoff'])\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        teams_sorted[col] = (\n",
    "            teams_sorted.groupby('tmID')[col]\n",
    "            .rolling(window=i, min_periods=1, closed='left')\n",
    "            .apply(weighted_mean, raw=False)\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "\n",
    "    # If first appearance of team on wnba, fill the stats with the average of the other rows\n",
    "    # Calculate the mean of each numeric column\n",
    "    average_numeric = teams_sorted.select_dtypes(include=np.number).mean()\n",
    "\n",
    "    # Fill NaN values in numeric columns with their means\n",
    "    teams_sorted.loc[:, teams_sorted.dtypes == np.number] = teams_sorted.loc[:, teams_sorted.dtypes == np.number].fillna(average_numeric)\n",
    "\n",
    "\n",
    "    df = teams_sorted.merge(team_players_year_stats, on=['tmID', 'year'], how='left', suffixes=('', f'_{i}'))\n",
    "\n",
    "    df = df[df['year'] != 1]\n",
    "    df = df.round(2)\n",
    "\n",
    "    df.to_csv('../prep_data/rolling_window/data_with_' + str(i) + '_years_in_the_past.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2254125da0ad1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
