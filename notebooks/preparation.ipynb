{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a474e2",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb42898",
   "metadata": {},
   "source": [
    "## Database Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aeeee5",
   "metadata": {},
   "source": [
    "We used a free service to host our database. The Database is in PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Credentials\n",
    "\n",
    "with open(\"../config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "host = config[\"db_host\"]\n",
    "user = config[\"db_user\"]\n",
    "password = config[\"db_password\"]\n",
    "database = config[\"db_database\"]\n",
    "schema = config[\"db_schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "def execute(query):\n",
    "    cursor.execute(query)\n",
    "    connection.commit()\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def fetch(query):\n",
    "    cursor.execute(query)\n",
    "    return cursor.fetchall()\n",
    "\n",
    "SELECT = \"SELECT * FROM \" + schema + \".\" # + table_name \n",
    "INSERT = \"INSERT INTO \" + schema + \".\" # + table_name + \" VALUES \" + values\n",
    "UPDATE = \"UPDATE \" + schema + \".\" # + table_name + \" SET \" + column_name + \" = \" + value\n",
    "DELETE = \"DELETE FROM \" + schema + \".\"  # + table_name + \" WHERE \" + column_name + \" = \" + value\n",
    "\n",
    "# Test data, year 10\n",
    "TEST_DATA = \"year = 10\"\n",
    "TRAIN_DATA = \"year < 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44416216",
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_players = fetch(SELECT + \"awards_players\") # awards and prizes received by players across 10 seasons,\n",
    "coaches = fetch(SELECT + \"coaches\") # all coaches who've managed the teams during the time period,\n",
    "players = fetch(SELECT + \"players\") # details of all players,\n",
    "players_teams = fetch(SELECT + \"players_teams\") # performance of each player for each team they played,\n",
    "series_post = fetch(SELECT + \"series_post\") # series' results,\n",
    "teams = fetch(SELECT + \"teams\") # performance of the teams for each season,\n",
    "teams_post = fetch(SELECT + \"teams_post\") # results of each team at the post-season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4454347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data in a dataframe\n",
    "awards_players_df = pd.DataFrame(awards_players, columns=['playerID', 'award', 'year', 'lgID'])\n",
    "coaches_df = pd.DataFrame(coaches, columns=['coachID', 'year', 'tmID', 'lgID', 'stint', 'won', 'lost', 'post_wins', 'post_losses'])\n",
    "players_df = pd.DataFrame(players, columns=['bioID', 'pos', 'firstseason', 'lastseason', 'height', 'weight', 'college', 'collegeOther', 'birthDate', 'deathDate'])\n",
    "players_teams_df = pd.DataFrame(players_teams, columns=['playerID', 'year', 'stint', 'tmID', 'lgID', 'GP', 'GS', 'minutes', 'points', 'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade', 'dq', 'PostGP', 'PostGS', 'PostMinutes', 'PostPoints', 'PostoRebounds', 'PostdRebounds', 'PostRebounds', 'PostAssists', 'PostSteals', 'PostBlocks', 'PostTurnovers', 'PostPF', 'PostfgAttempted', 'PostfgMade', 'PostftAttempted', 'PostftMade', 'PostthreeAttempted', 'PostthreeMade', 'PostDQ'])\n",
    "series_post_df = pd.DataFrame(series_post, columns=['year', 'round', 'series', 'tmIDWinner', 'lgIDWinner', 'tmIDLoser', 'lgIDLoser', 'W', 'L'])\n",
    "teams_df = pd.DataFrame(teams, columns=['year', 'lgID', 'tmID', 'franchID', 'confID', 'divID', 'rank', 'playoff', 'seeded', 'firstRound', 'semis', 'finals', 'name', 'o_fgm', 'o_fga', 'o_ftm', 'o_fta', 'o_3pm', 'o_3pa', 'o_oreb', 'o_dreb', 'o_reb', 'o_asts', 'o_pf', 'o_stl', 'o_to', 'o_blk', 'o_pts', 'd_fgm', 'd_fga', 'd_ftm', 'd_fta', 'd_3pm', 'd_3pa', 'd_oreb', 'd_dreb', 'd_reb', 'd_asts', 'd_pf', 'd_stl', 'd_to', 'd_blk', 'd_pts', 'tmORB', 'tmDRB', 'tmTRB', 'opptmORB', 'opptmDRB', 'opptmTRB', 'won', 'lost', 'GP', 'homeW', 'homeL', 'awayW', 'awayL', 'confW', 'confL', 'min', 'attend', 'arena'])\n",
    "teams_post_df = pd.DataFrame(teams_post, columns=['year', 'tmID', 'lgID', 'W', 'L'])\n",
    "\n",
    "#make a dictionary with all the dataframes\n",
    "dfs = {'awards_players_df': awards_players_df, 'coaches_df': coaches_df, 'players_df': players_df, 'players_teams_df': players_teams_df, 'series_post_df': series_post_df, 'teams_df': teams_df, 'teams_post_df': teams_post_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17109c4",
   "metadata": {},
   "source": [
    "So with that we end our understanding phase.\n",
    "Our main takeaways are:\n",
    "- There are dead players in the players table. We should take that into account when doing the analysis.\n",
    "- There are players that have not played any season of the seasons given. We should take that into account when doing the analysis. There are 338 players that have not played any season.\n",
    "- There are no Null entries (although there values that are simply an empty string)\n",
    "- There are some columns with the DataType \"object\", most of them being strings.\n",
    "- There are binary objects (like confID and playoff, in the 'teams' table, with the values \"Y\" or \"N\") that could be substituted by a binary, as well as ternary objects (like the firstRound, semis and finals in the 'teams' table, with the values \"W\", \"L\" or \"\") that could also be transformed.\n",
    "- There are players with no position and no college assigned (\"\").\n",
    "- There are players with no date of birth in the record (0000-00-00).\n",
    "- There is the need to do null value uniformization, as there are some columns with empty strings, others with default 0 values and other values that represent null.\n",
    "- The height and weight variables have default 0 values and should be treated as null values.\n",
    "- The number of games played by each team differs (there may be teams that are no longer playing), so we can't compare the number of wins and losses directly. Win percentage should be used.\n",
    "- In terms of win percentage, it seems like a competitive league, with more than half of the teams having a win percentage of 50% or more, taking advantage of the worst teams. There is also just one team below 40% of wins.\n",
    "- There are teams that are no longer playing.\n",
    "- There are a lot of highly correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601ae40",
   "metadata": {},
   "source": [
    "## Preparing the data for the model\n",
    "\n",
    "In this notebook, we will prepare the data for the model. Having done the understanding in the [previous notebook](understanding.ipynb), we will now prepare the data for the model. From the understanding we came to the following conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642023b",
   "metadata": {},
   "source": [
    "So with that we end our understanding phase.\n",
    "Our main takeaways are:\n",
    "- There are dead players in the players table. We should take that into account when doing the analysis.\n",
    "- There are players that have not played any season of the seasons given. We should take that into account when doing the analysis. There are 338 players that have not played any season.\n",
    "- There are no Null entries (although there values that are simply an empty string)\n",
    "- There are some columns with the DataType \"object\", most of them being strings.\n",
    "- There are binary objects (like confID and playoff, in the 'teams' table, with the values \"Y\" or \"N\") that could be substituted by a binary, as well as ternary objects (like the firstRound, semis and finals in the 'teams' table, with the values \"W\", \"L\" or \"\") that could also be transformed.\n",
    "- There are players with no position and no college assigned (\"\").\n",
    "- There are players with no date of birth in the record (0000-00-00).\n",
    "- There is the need to do null value uniformization, as there are some columns with empty strings, others with default 0 values and other values that represent null.\n",
    "- The height and weight variables have default 0 values and should be treated as null values.\n",
    "- The number of games played by each team differs (there may be teams that are no longer playing), so we can't compare the number of wins and losses directly. Win percentage should be used.\n",
    "- In terms of win percentage, it seems like a competitive league, with more than half of the teams having a win percentage of 50% or more, taking advantage of the worst teams. There is also just one team below 40% of wins.\n",
    "- There are teams that are no longer playing.\n",
    "- There are a lot of highly correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90834ca",
   "metadata": {},
   "source": [
    "After considering our takeaways, we will now prepare the data for the model. We will do the following:\n",
    "- Remove the players that have not played any season, and, if a player died, remove the seasons after the death.\n",
    "- Transform the binary objects into binary values.\n",
    "- Transform the ternary objects into binary values. (where the third value is a null value - after the null uniformization these are considered as binary objects too)\n",
    "- Null uniformization: transform the empty strings and default 0 values into null values.\n",
    "- Analysis null values: analyze the null values and decide what to do with them.\n",
    "- Calculate win percentage for each team and add it to the teams table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2ce92",
   "metadata": {},
   "source": [
    "We begin by excluding columns that consistently have identical values since they do not contribute any valuable information to the model. However, we will retain the 'first season' and 'last season' of a player, as we intend to populate them with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# merge teams_df with players_df\n",
    "awards_players_df = pd.read_csv('../prep_data/awards_players_df.csv')\n",
    "coaches_df = pd.read_csv('../prep_data/coaches_df.csv')\n",
    "players_df = pd.read_csv('../prep_data/players_df.csv')\n",
    "players_teams_df = pd.read_csv('../prep_data/players_teams_df.csv')\n",
    "series_post_df = pd.read_csv('../prep_data/series_post_df.csv')\n",
    "teams_df = pd.read_csv('../prep_data/teams_df.csv')\n",
    "teams_post_df = pd.read_csv('../prep_data/teams_post_df.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ce88152b96deb2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a375c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns whose values are always the same\n",
    "for df in dfs:\n",
    "    for col in dfs[df].columns:\n",
    "        if len(dfs[df][col].unique()) == 1 and col not in ['firstseason', 'lastseason'] :\n",
    "            print(df, col)\n",
    "            dfs[df].drop(col, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7679878",
   "metadata": {},
   "source": [
    "### Null uniformization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9448bff",
   "metadata": {},
   "source": [
    "We identified the following columns that have null values, but are not identified as such:\n",
    "- players: height, weight, birthDate, position, college, deathDate\n",
    "- teams: firstRound, semis, finals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941273cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If date == 00-00-00, replace with null (birthDate and deathDate)\n",
    "\n",
    "dfs[\"players_df\"][\"birthDate\"] = dfs[\"players_df\"][\"birthDate\"].replace('00-00-00', None)\n",
    "dfs[\"players_df\"][\"birthDate\"] = dfs[\"players_df\"][\"birthDate\"].replace('0000-00-00', None)\n",
    "dfs[\"players_df\"][\"deathDate\"] = dfs[\"players_df\"][\"deathDate\"].replace('00-00-00', None)\n",
    "dfs[\"players_df\"][\"deathDate\"] = dfs[\"players_df\"][\"deathDate\"].replace('0000-00-00', None)\n",
    "\n",
    "# If value == 0, replace with null (height, weight)\n",
    "\n",
    "dfs[\"players_df\"][\"height\"] = dfs[\"players_df\"][\"height\"].replace(0, None)\n",
    "dfs[\"players_df\"][\"weight\"] = dfs[\"players_df\"][\"weight\"].replace(0, None)\n",
    "\n",
    "# If value == \"\", replace with null (college, collegeOther, firstRound, semis, finals)\n",
    "\n",
    "dfs[\"players_df\"][\"college\"] = dfs[\"players_df\"][\"college\"].replace('', None)\n",
    "dfs[\"players_df\"][\"collegeOther\"] = dfs[\"players_df\"][\"collegeOther\"].replace('', None)\n",
    "dfs[\"teams_df\"][\"firstRound\"] = dfs[\"teams_df\"][\"firstRound\"].replace('', None)\n",
    "dfs[\"teams_df\"][\"semis\"] = dfs[\"teams_df\"][\"semis\"].replace('', None)\n",
    "dfs[\"teams_df\"][\"finals\"] = dfs[\"teams_df\"][\"finals\"].replace('', None)\n",
    "\n",
    "dfs[\"players_df\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907828e",
   "metadata": {},
   "source": [
    "### Remove the players that have not played any season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#players that have not played in the last 10 years\n",
    "players_not_played = fetch(\"SELECT p.bioid FROM wnba.players p WHERE p.bioid not in (select pt.playerid  from wnba.players_teams pt)\")\n",
    "print(\"Number of players that haven't played: \" + \n",
    "      str(len(players_not_played)))\n",
    "\n",
    "players_not_played_df = pd.DataFrame(players_not_played, columns=['bioID'])\n",
    "\n",
    "players_not_played_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861dba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the number of players that have not played in the last 10 years, and the lenght of the 3 dataframes that contain the playerID\n",
    "print(\"Number of players that have not played: \", len(players_not_played_df['bioID'].unique()))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Number of values in the players_team_df: \", len(dfs['players_teams_df']['playerID'].unique()))\n",
    "print(\"Number of values in the awards_players_df: \", len(dfs['awards_players_df']['playerID'].unique()))\n",
    "print(\"Number of values in the players_df: \", len(dfs['players_df']['bioID'].unique()))\n",
    "\n",
    "#Remove the players that have not played in the last 10 years\n",
    "for df in dfs:\n",
    "    if(df == 'players_teams_df' or df == 'awards_players_df'):\n",
    "        dfs[df] = dfs[df][~dfs[df]['playerID'].isin(players_not_played_df['bioID'])]\n",
    "    if(df == 'players_df'):\n",
    "        dfs[df] = dfs[df][~dfs[df]['bioID'].isin(players_not_played_df['bioID'])]\n",
    "\n",
    "#Print the number of players that have not played in the last 10 years, and the lenght of the 3 dataframes that contain the playerID\n",
    "print('\\n')\n",
    "print(\"Number of values in the players_team_df: \", len(dfs['players_teams_df']['playerID'].unique()))\n",
    "print(\"Number of values in the awards_players_df: \", len(dfs['awards_players_df']['playerID'].unique()))\n",
    "print(\"Number of values in the players_df: \", len(dfs['players_df']['bioID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82108b56",
   "metadata": {},
   "source": [
    "### Populate first and last seasons of a player in the wnba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac6861",
   "metadata": {},
   "source": [
    "As we mentioned before we will populate first and last season of a player in the wnba. We will do this by looking at the seasons table and finding the first and last season of a player. We will then populate the first and last season of a player in the players table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the players_teams_df by 'playerID' to find the first and last seasons.\n",
    "first_seasons = dfs['players_teams_df'].groupby('playerID')['year'].min()\n",
    "last_seasons = dfs['players_teams_df'].groupby('playerID')['year'].max()\n",
    "\n",
    "# Use .loc to set the values in players_df without the warning.\n",
    "dfs['players_df'].loc[:, 'firstseason'] = dfs['players_df']['bioID'].map(first_seasons)\n",
    "dfs['players_df'].loc[:, 'lastseason'] = dfs['players_df']['bioID'].map(last_seasons)\n",
    "\n",
    "print(dfs['players_df'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304bca3",
   "metadata": {},
   "source": [
    "### Transform the binary objects into binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the binary columns from all the dataframes\n",
    "binary_columns = []\n",
    "for df in dfs:\n",
    "    binary_columns = binary_columns + [(df, list(dfs[df].columns[dfs[df].nunique() == 2]))]\n",
    "\n",
    "#Print the binary columns uniques values\n",
    "for i in binary_columns:\n",
    "    if(len(i[1]) < 0):\n",
    "        continue\n",
    "\n",
    "    for j in i[1]:\n",
    "        print(\"-------\")\n",
    "        print(i[0], j)\n",
    "        print(dfs[i[0]][j].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0f9b3",
   "metadata": {},
   "source": [
    "Death date being a binary value is a coincidence, as only one player (that has played in the seasons we have) has died.\n",
    "GP is the number of games played and should also not be converted to binary, as we will need this value to calculate the win percentage. (it is only binary because seasons have been of 32 games or 34 games). The W value in the series_post represents the number of wins a team winned in the playoffs. All the playoffs games are in the best of 3 or 5, so the winning team wins 2 or 3 games.\n",
    "The other binary values are binary and should be converted to binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the binary columns to 0 and 1 (confID, playoff, firstRound, semis, finals)\n",
    "\n",
    "binary_columns = [\"confID\", \"playoff\", \"firstRound\", \"semis\", \"finals\"]\n",
    "\n",
    "for col in binary_columns:\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('EA', 0)\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('WE', 1)\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('L', 0)\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('W', 1)\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('N', 0)\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].replace('Y',1)\n",
    "    #change the type of the column to int\n",
    "    dfs[\"teams_df\"][col] = dfs[\"teams_df\"][col].astype(\"Int64\")\n",
    "\n",
    "dfs[\"teams_df\"].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093bcae",
   "metadata": {},
   "source": [
    "### Calculate win percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01b188f",
   "metadata": {},
   "source": [
    "For each team, we want to calculate the following:\n",
    "- Win percentage\n",
    "- Loss percentage\n",
    "- Wins at home percentage\n",
    "- Losses at home percentage\n",
    "- Wins away percentage\n",
    "- Losses away percentage\n",
    "- Conference wins percentage\n",
    "- Conference losses percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8cf7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate win percentage, loss percentage, wins at home percentage, losses at home percentage, wins away percentage, losses away percentage, wins at conference percentage, losses at conference percentage\n",
    "\n",
    "dfs[\"teams_df\"][\"win_percentage\"] = dfs[\"teams_df\"][\"won\"] / (dfs[\"teams_df\"][\"won\"] + dfs[\"teams_df\"][\"lost\"])\n",
    "dfs[\"teams_df\"][\"loss_percentage\"] = dfs[\"teams_df\"][\"lost\"] / (dfs[\"teams_df\"][\"won\"] + dfs[\"teams_df\"][\"lost\"])\n",
    "dfs[\"teams_df\"][\"home_win_percentage\"] = dfs[\"teams_df\"][\"homeW\"] / (dfs[\"teams_df\"][\"homeW\"] + dfs[\"teams_df\"][\"homeL\"])\n",
    "dfs[\"teams_df\"][\"home_loss_percentage\"] = dfs[\"teams_df\"][\"homeL\"] / (dfs[\"teams_df\"][\"homeW\"] + dfs[\"teams_df\"][\"homeL\"])\n",
    "dfs[\"teams_df\"][\"away_win_percentage\"] = dfs[\"teams_df\"][\"awayW\"] / (dfs[\"teams_df\"][\"awayW\"] + dfs[\"teams_df\"][\"awayL\"])\n",
    "dfs[\"teams_df\"][\"away_loss_percentage\"] = dfs[\"teams_df\"][\"awayL\"] / (dfs[\"teams_df\"][\"awayW\"] + dfs[\"teams_df\"][\"awayL\"])\n",
    "dfs[\"teams_df\"][\"conference_win_percentage\"] = dfs[\"teams_df\"][\"confW\"] / (dfs[\"teams_df\"][\"confW\"] + dfs[\"teams_df\"][\"confL\"])\n",
    "dfs[\"teams_df\"][\"conference_loss_percentage\"] = dfs[\"teams_df\"][\"confL\"] / (dfs[\"teams_df\"][\"confW\"] + dfs[\"teams_df\"][\"confL\"])\n",
    "\n",
    "#Drop the columns that are not needed anymore\n",
    "dfs[\"teams_df\"] = dfs[\"teams_df\"].drop(columns=['won', 'lost', 'homeW', 'homeL', 'awayW', 'awayL', 'confW', 'confL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e082e",
   "metadata": {},
   "source": [
    "## Data Preparation on players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef549740",
   "metadata": {},
   "source": [
    "### Position Uniformization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c8427",
   "metadata": {},
   "source": [
    "From the list below we can see that there are 7 different positions. We will uniformize the positions to the following:\n",
    "- Guard (G)\n",
    "- Forward (F)\n",
    "- Center (C)\n",
    "- Guard-Forward (G-F)\n",
    "- Forward-Center (F-C)\n",
    "\n",
    "But, as we can see from the distinct positions, we have 2 more positions that are not in the list above. These are:\n",
    "(C-F) and (F-G). We will uniformize these positions to the ones above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_positions = dfs['players_df']['pos'].unique()\n",
    "print(unique_positions)\n",
    "\n",
    "# Define specific_position_mapping\n",
    "specific_position_mapping = {\n",
    "    'F-G': 'G-F',\n",
    "    'C-F': 'F-C'\n",
    "}\n",
    "\n",
    "# Use .loc to update the 'pos' column in players_df\n",
    "dfs['players_df'].loc[:, 'pos'] = dfs['players_df'].loc[:, 'pos'].replace(specific_position_mapping)\n",
    "\n",
    "# Check the unique values after mapping\n",
    "print(\"After mapping\")\n",
    "unique_positions = dfs['players_df']['pos'].unique()\n",
    "print(unique_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d59c60",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded1942",
   "metadata": {},
   "source": [
    "We will now prepare the data for the players table. We will do the following:\n",
    "- Feature engineering: create a column with the number of seasons a player played in the wnba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['players_df'].loc[:, 'num_seasons'] = dfs['players_df']['lastseason'] - dfs['players_df']['firstseason'] + 1\n",
    "\n",
    "dfs['players_df'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b28d9",
   "metadata": {},
   "source": [
    "- Total Points in the season\n",
    "- Total Rebounds in the season\n",
    "- Total Assists in the season\n",
    "- Total Steals in the season\n",
    "- Total Turnovers in the season\n",
    "- Total Goal Percentage in the season\n",
    "- Total Three Point Percentage in the season\n",
    "- Total Free Throw Percentage in the season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9673ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['players_teams_df']['total_points'] = dfs['players_teams_df']['points'] + dfs['players_teams_df']['PostPoints']\n",
    "dfs['players_teams_df']['total_rebounds'] = dfs['players_teams_df']['rebounds'] + dfs['players_teams_df']['PostRebounds']\n",
    "dfs['players_teams_df']['total_assists'] = dfs['players_teams_df']['assists'] + dfs['players_teams_df']['PostAssists']\n",
    "dfs['players_teams_df']['total_blocks'] = dfs['players_teams_df']['blocks'] + dfs['players_teams_df']['PostBlocks']\n",
    "dfs['players_teams_df']['total_steals'] = dfs['players_teams_df']['steals'] + dfs['players_teams_df']['PostSteals']\n",
    "dfs['players_teams_df']['total_turnovers'] = dfs['players_teams_df']['turnovers'] + dfs['players_teams_df']['PostTurnovers']\n",
    "dfs['players_teams_df']['FG%'] = (dfs['players_teams_df']['fgMade'] + dfs['players_teams_df']['PostfgMade']) / (dfs['players_teams_df']['fgAttempted'] + dfs['players_teams_df']['PostfgAttempted'])\n",
    "dfs['players_teams_df']['FT%'] = (dfs['players_teams_df']['ftMade'] + dfs['players_teams_df']['PostftMade']) / (dfs['players_teams_df']['ftAttempted'] + dfs['players_teams_df']['PostftAttempted'])\n",
    "\n",
    "# Average Stats\n",
    "dfs['players_teams_df']['Avg_Points_Per_Game'] = dfs['players_teams_df']['total_points'] / dfs['players_teams_df']['GP']\n",
    "dfs['players_teams_df']['Avg_Rebounds_Per_Game'] = dfs['players_teams_df']['total_rebounds'] / dfs['players_teams_df']['GP']\n",
    "dfs['players_teams_df']['Avg_Assists_Per_Game'] = dfs['players_teams_df']['total_assists'] / dfs['players_teams_df']['GP']\n",
    "dfs['players_teams_df']['Avg_Blocks_Per_Game'] = dfs['players_teams_df']['total_blocks'] / dfs['players_teams_df']['GP']\n",
    "dfs['players_teams_df']['Avg_Steals_Per_Game'] = dfs['players_teams_df']['total_steals'] / dfs['players_teams_df']['GP']\n",
    "dfs['players_teams_df']['Avg_Turnovers_Per_Game'] = dfs['players_teams_df']['total_turnovers'] / dfs['players_teams_df']['GP']\n",
    "\n",
    "\n",
    "# Ratio of Points in Playoffs to Regular Season\n",
    "dfs['players_teams_df']['Points_Ratio'] = dfs['players_teams_df']['PostPoints'] / dfs['players_teams_df']['points']\n",
    "\n",
    "\n",
    "# Durability Ratio (Check if this is correct)\n",
    "dfs['players_teams_df']['Durability_Ratio'] = (dfs['players_teams_df']['minutes'] + dfs['players_teams_df']['PostMinutes']) / (dfs['players_teams_df']['GP'] * 48)\n",
    "\n",
    "# Get the player position from dfs['players']['pos'], match ['bioid'] to ['playerID'] in dfs['players_teams_df']\n",
    "position_mapping = dfs['players_df'].set_index('bioID')['pos']\n",
    "dfs['players_teams_df']['pos'] = dfs['players_teams_df']['playerID'].map(position_mapping)\n",
    "\n",
    "\n",
    "# Define a dictionary of position-specific metrics\n",
    "position_metrics = {\n",
    "    'G': 'AST_TO_RATIO',\n",
    "    'F': 'REBOUND_EFFICIENCY',\n",
    "    'G-F': '3P_SHOOTING_PERCENT',\n",
    "    'C': 'BLOCK_EFFICIENCY',\n",
    "    'F-C': 'SCORING_EFFICIENCY'\n",
    "}\n",
    "\n",
    "# Calculate position-specific metrics and update the DataFrame\n",
    "for position, metric in position_metrics.items():\n",
    "    position_df = dfs['players_teams_df'][dfs['players_teams_df']['pos'] == position]\n",
    "    dfs['players_teams_df'][metric] = position_df.apply(\n",
    "        lambda row: row['assists'] / row['turnovers'] if metric == 'AST_TO_RATIO' and row['turnovers'] != 0 else\n",
    "                    (row['oRebounds'] + row['dRebounds']) / row['GP'] if metric == 'REBOUND_EFFICIENCY' and row['GP'] != 0 else\n",
    "                    row['threeMade'] / row['threeAttempted'] if metric == '3P_SHOOTING_PERCENT' and row['threeAttempted'] != 0 else\n",
    "                    row['blocks'] / row['GP'] if metric == 'BLOCK_EFFICIENCY' and row['GP'] != 0 else\n",
    "                    row['points'] / row['fgAttempted'] if metric == 'SCORING_EFFICIENCY' and row['fgAttempted'] != 0 else\n",
    "                    None, axis=1\n",
    "    )\n",
    "\n",
    "def extract_first_non_null(row):\n",
    "    for column in list(position_metrics.values()):\n",
    "        if not pd.isnull(row[column]):\n",
    "            return row[column]\n",
    "    return None\n",
    "\n",
    "dfs['players_teams_df']['POSITION_METRIC'] = dfs['players_teams_df'].apply(extract_first_non_null, axis=1)\n",
    "dfs['players_teams_df'].drop(columns=list(position_metrics.values()), inplace=True)\n",
    "\n",
    "\n",
    "dfs['players_teams_df'].columns\n",
    "dfs['players_teams_df'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f739094",
   "metadata": {},
   "source": [
    "### Check correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b23916",
   "metadata": {},
   "source": [
    "We will remove the most correlated variables, as they do not add any value to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CORRELATION = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column to indicate if the player went to the playoffs to look for correlations\n",
    "def went_to_playoff(df, dfs):\n",
    "\n",
    "    returned_df = df.copy()\n",
    "    \n",
    "    playoff_data = dfs['teams_df'][['tmID', 'year', 'playoff']]\n",
    "\n",
    "    returned_df = returned_df.merge(playoff_data, on=['tmID', 'year'], how='left')\n",
    "    \n",
    "    return returned_df\n",
    "\n",
    "def delete_most_correlated(df):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "\n",
    "    correlation_matrix = df_copy.corr()\n",
    "\n",
    "    sorted_correlations = correlation_matrix.unstack().sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "    # Get the pairs of attributes with the highest correlation values\n",
    "    most_correlated_pairs = sorted_correlations[sorted_correlations > MAX_CORRELATION]\n",
    "    most_correlated_pairs = most_correlated_pairs[most_correlated_pairs < 1.0]\n",
    "\n",
    "\n",
    "    #delete repeated pairs (e.g. (a,b) and (b,a))\n",
    "    most_correlated_pairs = most_correlated_pairs[::2]\n",
    "    print(most_correlated_pairs)\n",
    "\n",
    "    #drop the attributes with the highest correlation values\n",
    "    for pair in most_correlated_pairs.index:\n",
    "        if pair[1] in df_copy.columns and pair[1] not in ['year']:\n",
    "            df_copy.drop(pair[1], inplace=True, axis=1)\n",
    "\n",
    "    #print the attributes that were dropped\n",
    "    print(df_copy.columns)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618cb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    print(df)\n",
    "    if 'players_teams_df' in df:\n",
    "        dfs[df] = went_to_playoff(dfs[df], dfs)\n",
    "    #select only the numerical columns\n",
    "    new_df = delete_most_correlated(dfs[df].select_dtypes(include=np.number))\n",
    "    #merge new_df with the categorical columns\n",
    "    print(new_df)\n",
    "    dfs[df] = new_df.merge(dfs[df].select_dtypes(exclude=np.number), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a02228",
   "metadata": {},
   "source": [
    "### TODO: NEED TO MERGE INFORMATION FROM DIFFERENT TABLES, BUT HOW?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42d618",
   "metadata": {},
   "source": [
    "### Remove categorical variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb7970",
   "metadata": {},
   "source": [
    "There are several categorical variables that we will remove from the model, as they do not add any value to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove categorical columns from team_df that are not needed \n",
    "#franchID, name, arena\n",
    "\n",
    "dfs['teams_df'] = dfs['teams_df'].drop(columns=['franchID', 'name', 'arena'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d1451",
   "metadata": {},
   "source": [
    "### Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ffae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the tables in a csv file, inside the prep_data folder\n",
    "for df in dfs:\n",
    "    dfs[df].to_csv('../prep_data/' + df + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalization and Standardization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b6be1c673254a25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433b7372df252d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the numeric columns in players_teams_df verify if they follow gaussian distribution\n",
    "from scipy import stats\n",
    "for col in dfs['players_teams_df'].select_dtypes(include=np.number).columns:\n",
    "    # print(dfs['players_teams_df'][col].dtype)\n",
    "    # print('jrj')\n",
    "    # sns.distplot(dfs['players_teams_df'][col])\n",
    "    # plt.show()\n",
    "    \n",
    "    # evaluate if the column follows a gaussian distribution\n",
    "    \n",
    "    statistics, p_value = stats.shapiro(dfs['players_teams_df'][col])\n",
    "    alpha = 0.05\n",
    "    if p_value > alpha:\n",
    "        print('{} follows a Gaussian distribution'.format(col))\n",
    "        \n",
    "    # print('Statistics=%.3f, p=%.3f' % (statistics, p_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are no Gaussian distributions in the numeric columns of the players_teams_df, so we will only apply linear normalizations between 0 and 1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35dc7a096c303016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for the numeric columns in players_teams_df apply linear normalization between 0 and 1\n",
    "for col in dfs['teams_df'].select_dtypes(include=np.number).columns:\n",
    "    if col not in ['year', 'confID', 'playoff', 'firstRound', 'semis', 'finals']:\n",
    "        aux = (dfs['teams_df'][col] - dfs['teams_df'][col].min()) / (dfs['teams_df'][col].max() - dfs['teams_df'][col].min())\n",
    "        dfs['teams_df'][col] = aux.round(3)\n",
    "# store in csv in prep_data with 2 appended to the name\n",
    "dfs['teams_df'].to_csv('../prep_data/teams_df2.csv', index=False)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78ca4d928702f98f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merging Tables (FALTAM COISAS ANTES MAS VOU VER SE DÀ PARA IR PENSANDO NISTO)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b904cfca0c968389"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merged tables:\n",
    "- teams with teams_post, some values are NaN when the team didn't participate in the playoffs\n",
    "- players with players_teams, some values are NaN when the player didn't participate in any team, remove such cases?\n",
    "- awards with players\n",
    "\n",
    "- próximas ideia... listas de valores agregados ou colunas a 0 ou 1 se forem poucos valores diferentes\n",
    "- acrescentar coluna com pontuação a cada jogador e depois somar essas pontuações em teams\n",
    "- Is series_post even relevant??\n",
    "- se se mergirem os coaches, fazer a média dos resultados quando há mais q 1 por ano por equipa? criar alguma espécie de pontuação em vez das colunas todas?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d0e26b4153ef79e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "teams_post_df = teams_post_df.rename(columns={'W': 'W_post', 'L': 'L_post'})\n",
    "teams_df = teams_df.merge(teams_post_df, on=['tmID', 'year'], how='left')\n",
    "\n",
    "teams_df.head()\n",
    "# teams_df.to_csv('teams_df_merge_with_post.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b815602a660f467d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# join players (bioID) with players_teams (playerID)\n",
    "players_teams_df = players_teams_df.rename(columns={'playerID': 'bioID'})\n",
    "players_teams_df.head()\n",
    "players_df = players_df.merge(players_teams_df, on=['bioID', 'pos'], how='right')\n",
    "players_df = players_df.rename(columns={'bioID': 'playerID'})\n",
    "players_df.head()\n",
    "\n",
    "# to csv\n",
    "# players_df.to_csv('players_df_merge_with_players_teams.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cd4da2e391f2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# awards_players_df.head()\n",
    "players_df.head()\n",
    "# rename bioID to playerID\n",
    "players_df = players_df.rename(columns={'bioID': 'playerID'})\n",
    "players_df = players_df.merge(awards_players_df, on=['playerID', 'year'], how='left')\n",
    "\n",
    "# players_df.to_csv('players_df_merge_with_awards.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54d2e2be4b877924"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "players_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71f05ad41275148c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# for each team in teams_df, calculate the average of each player's performance in the team, for every column in players_df\n",
    "teams_df3 = teams_df.copy()\n",
    "for col in players_df:\n",
    "    col_type = players_df[col].dtype\n",
    "    if col_type == 'float64' or col_type == 'int64' and col not in {'year', 'tmID', 'lgID', 'playerID', 'firstseason', 'lastseason', 'playoff'}:\n",
    "        grouped = (players_df.groupby(['year', 'tmID', 'playoff'])[col].mean())\n",
    "        grouped = grouped.reset_index()\n",
    "\n",
    "#     add the average to teams_df\n",
    "        teams_df3 = teams_df3.merge(grouped, on=['year', 'tmID', 'playoff'], how='left')\n",
    "        \n",
    "        \n",
    "#         in column Points_Ratio replace inf with 0\n",
    "teams_df3['Points_Ratio'] = teams_df3['Points_Ratio'].replace([np.inf, -np.inf], 0)\n",
    "teams_df3.to_csv('../prep_data/teams_df3.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5bea00beb86d65b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#do similarly but now instead of the average of each player's performance, calculate the sum of the 3 best players' performance\n",
    "teams_df4 = teams_df.copy()\n",
    "for col in players_df:\n",
    "    col_type = players_df[col].dtype\n",
    "    if col_type == 'float64' or col_type == 'int64' and col not in {'year', 'tmID', 'lgID', 'playerID', 'firstseason', 'lastseason', 'playoff'}:\n",
    "        grouped = players_df.groupby(['year', 'tmID', 'playoff'])[col]\n",
    "        grouped = grouped.apply(lambda x: x.nlargest(3).sum())\n",
    "        \n",
    "        grouped = grouped.reset_index()\n",
    "        \n",
    "#     add the average to teams_df\n",
    "        teams_df4 = teams_df4.merge(grouped, on=['year', 'tmID', 'playoff'], how='left')\n",
    "        \n",
    "#         in column Points_Ratio replace inf with 0\n",
    "teams_df4['Points_Ratio'] = teams_df4['Points_Ratio'].replace([np.inf, -np.inf], 0)\n",
    "teams_df4.to_csv('../prep_data/teams_df4.csv', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a5ac5909f313725"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2254125da0ad1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896a31c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
